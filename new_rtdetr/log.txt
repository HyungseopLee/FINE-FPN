W0626 22:12:27.485000 66588 site-packages/torch/distributed/run.py:766] 
W0626 22:12:27.485000 66588 site-packages/torch/distributed/run.py:766] *****************************************
W0626 22:12:27.485000 66588 site-packages/torch/distributed/run.py:766] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0626 22:12:27.485000 66588 site-packages/torch/distributed/run.py:766] *****************************************
RANK: 1, LOCAL_RANK: 1, WORLD_SIZE: 4
RANK: 2, LOCAL_RANK: 2, WORLD_SIZE: 4
RANK: 3, LOCAL_RANK: 3, WORLD_SIZE: 4
RANK: 0, LOCAL_RANK: 0, WORLD_SIZE: 4
/home/hslee/anaconda3/envs/torch271/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
/home/hslee/anaconda3/envs/torch271/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
/home/hslee/anaconda3/envs/torch271/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
/home/hslee/anaconda3/envs/torch271/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
[rank1]:[W626 22:12:30.740840143 ProcessGroupNCCL.cpp:4718] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can pecify device_id in init_process_group() to force use of a particular device.
[rank3]:[W626 22:12:30.747181009 ProcessGroupNCCL.cpp:4718] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can pecify device_id in init_process_group() to force use of a particular device.
[rank0]:[W626 22:12:30.747234902 ProcessGroupNCCL.cpp:4718] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can pecify device_id in init_process_group() to force use of a particular device.
[rank2]:[W626 22:12:30.747404636 ProcessGroupNCCL.cpp:4718] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can pecify device_id in init_process_group() to force use of a particular device.
Initialized distributed mode...
Initialized distributed mode...
Initialized distributed mode...
Initialized distributed mode...
cfg:  {'task': 'detection', '_model': None, '_postprocessor': None, '_criterion': None, '_optimizer': None, '_lr_scheduler': None, '_lr_warmup_scheduler': None, '_train_dataloader': None, '_val_dataloader': None, '_ema': None, '_scaler': None, '_train_dataset': None, '_val_dataset': None, '_collate_fn': None, '_evaluator': None, '_writer': None, 'num_workers': 0, 'batch_size': None, '_train_batch_size': None, '_val_batch_size': None, '_train_shuffle': None, '_val_shuffle': None, 'resume': None, 'tuning': None, 'epoches': 72, 'last_epoch': -1, 'use_amp': True, 'use_ema': True, 'ema_decay': 0.9999, 'ema_warmups': 2000, 'sync_bn': True, 'clip_max_norm': 0.1, 'find_unused_parameters': False, 'seed': 0, 'print_freq': 100, 'checkpoint_freq': 1, 'output_dir': './output/rtdetr_r50vd_6x_coco', 'summary_dir': None, 'device': '', 'yaml_cfg': {'task': 'detection', 'evaluator': {'type': 'CocoEvaluator', 'iou_types': ['bbox']}, 'num_classes': 80, 'remap_mscoco_category': True, 'train_dataloader': {'type': 'DataLoader', 'dataset': {'type': 'CocoDetection', 'img_folder': '/media/data/coco/train2017/', 'ann_file': '/media/data/coco/annotations/instances_train2017.json', 'return_masks': False, 'transforms': {'type': 'Compose', 'ops': [{'type': 'RandomPhotometricDistort', 'p': 0.5}, {'type': 'RandomZoomOut', 'fill': 0}, {'type': 'RandomIoUCrop', 'p': 0.8}, {'type': 'SanitizeBoundingBoxes', 'min_size': 1}, {'type': 'RandomHorizontalFlip'}, {'type': 'Resize', 'size': [640, 640]}, {'type': 'SanitizeBoundingBoxes', 'min_size': 1}, {'type': 'ConvertPILImage', 'dtype': 'float32', 'scale': True}, {'type': 'ConvertBoxes', 'fmt': 'cxcywh', 'normalize': True}]}}, 'shuffle': True, 'num_workers': 4, 'drop_last': True, 'collate_fn': {'type': 'BatchImageCollateFuncion', 'scales': [480, 512, 544, 576, 608, 640, 640, 640, 672, 704, 736, 768, 800]}, 'total_batch_size': 16}, 'val_dataloader': {'type': 'DataLoader', 'dataset': {'type': 'CocoDetection', 'img_folder': '/media/data/coco/val2017/', 'ann_file': '/media/data/coco/annotations/instances_val2017.json', 'return_masks': False, 'transforms': {'type': 'Compose', 'ops': [{'type': 'Resize', 'size': [640, 640]}, {'type': 'ConvertPILImage', 'dtype': 'float32', 'scale': True}]}}, 'shuffle': False, 'num_workers': 8, 'drop_last': False, 'collate_fn': {'type': 'BatchImageCollateFuncion'}, 'total_batch_size': 16}, 'print_freq': 100, 'output_dir': './output/rtdetr_r50vd_6x_coco', 'checkpoint_freq': 1, 'sync_bn': True, 'find_unused_parameters': False, 'use_amp': True, 'scaler': {'type': 'GradScaler', 'enabled': True}, 'use_ema': True, 'ema': {'type': 'ModelEMA', 'decay': 0.9999, 'warmups': 2000}, 'epoches': 72, 'clip_max_norm': 0.1, 'optimizer': {'type': 'AdamW', 'params': [{'params': '^(?=.*backbone)(?!.*(?:norm|bn)).*$', 'lr': 1e-05}, {'params': '^(?=.*backbone)(?=.*(?:norm|bn)).*$', 'weight_decay': 0.0, 'lr': 1e-05}, {'params': '^(?=.*(?:encoder|decoder))(?=.*(?:norm|bn|bias)).*$', 'weight_decay': 0.0}], 'lr': 0.0001, 'betas': [0.9, 0.999], 'weight_decay': 0.0001}, 'lr_scheduler': {'type': 'MultiStepLR', 'milestones': [1000], 'gamma': 0.1}, 'lr_warmup_scheduler': {'type': 'LinearWarmup', 'warmup_duration': 2000}, 'model': 'RTDETR', 'criterion': 'RTDETRCriterion', 'postprocessor': 'RTDETRPostProcessor', 'use_focal_loss': True, 'eval_spatial_size': [640, 640], 'RTDETR': {'backbone': 'PResNet', 'encoder': 'HybridEncoder', 'decoder': 'RTDETRTransformer'}, 'PResNet': {'depth': 50, 'variant': 'd', 'freeze_at': 0, 'return_idx': [1, 2, 3], 'num_stages': 4, 'freeze_norm': True, 'pretrained': True}, 'HybridEncoder': {'in_channels': [512, 1024, 2048], 'feat_strides': [8, 16, 32], 'hidden_dim': 256, 'use_encoder_idx': [2], 'num_encoder_layers': 1, 'nhead': 8, 'dim_feedforward': 1024, 'dropout': 0.0, 'enc_act': 'gelu', 'expansion': 1.0, 'depth_mult': 1, 'act': 'silu', 'version': 'v1'}, 'RTDETRTransformer': {'feat_channels': [256, 256, 256], 'feat_strides': [8, 16, 32], 'hidden_dim': 256, 'num_levels': 3, 'num_layers': 6, 'num_queries': 300, 'num_denoising': 100, 'label_noise_ratio': 0.5, 'box_noise_scale': 1.0, 'eval_idx': -1}, 'RTDETRPostProcessor': {'num_top_queries': 300}, 'RTDETRCriterion': {'weight_dict': {'loss_vfl': 1, 'loss_bbox': 5, 'loss_giou': 2}, 'losses': ['vfl', 'boxes'], 'alpha': 0.75, 'gamma': 2.0, 'matcher': {'type': 'HungarianMatcher', 'weight_dict': {'cost_class': 2, 'cost_bbox': 5, 'cost_giou': 2}, 'alpha': 0.25, 'gamma': 2.0}}, '__include__': ['../dataset/coco_detection.yml', '../runtime.yml', './include/dataloader.yml', './include/optimizer.yml', './include/rtdetr_r50vd.yml'], 'config': '/home/hslee/FINE-FPN/new_rtdetr/configs/rtdetr/rtdetr_r50vd_6x_coco.yml', 'seed': 0, 'test_only': False, 'print_method': 'builtin', 'print_rank': 0}}
Start training
Load PResNet50 state_dict
/home/hslee/FINE-FPN/new_rtdetr/tools/../src/core/workspace.py:179: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  return module(**module_kwargs)
/home/hslee/FINE-FPN/new_rtdetr/tools/../src/core/workspace.py:179: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  return module(**module_kwargs)
/home/hslee/FINE-FPN/new_rtdetr/tools/../src/core/workspace.py:179: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  return module(**module_kwargs)
/home/hslee/FINE-FPN/new_rtdetr/tools/../src/core/workspace.py:179: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  return module(**module_kwargs)
Initial lr: [1e-05, 1e-05, 0.0001, 0.0001]
building train_dataloader with batch_size=4...
loading annotations into memory...
Done (t=8.14s)
creating index...
index created!
building val_dataloader with batch_size=4...
loading annotations into memory...
Done (t=1.46s)
creating index...
index created!
number of trainable parameters: 42862860
Epoch: [0]  [   0/7393]  eta: 3:25:52  lr: 0.000000  loss: 44.4455 (44.4455)  loss_bbox: 1.4705 (1.4705)  loss_bbox_aux_0: 1.4788 (1.4788)  loss_bbox_aux_1: 1.5042 (1.5042)  loss_bbox_aux_2: 1.4879 (1.4879)  loss_bbox_aux_3: 1.4618 (1.4618)  loss_bbox_aux_4: 1.4850 (1.4850)  loss_bbox_aux_5: 1.5049 (1.5049)  loss_bbox_dn_0: 0.7877 (0.7877)  loss_bbox_dn_1: 0.7877 (0.7877)  loss_bbox_dn_2: 0.7877 (0.7877)  loss_bbox_dn_3: 0.7877 (0.7877)  loss_bbox_dn_4: 0.7877 (0.7877)  loss_bbox_dn_5: 0.7877 (0.7877)  loss_giou: 2.0750 (2.0750)  loss_giou_aux_0: 2.0892 (2.0892)  loss_giou_aux_1: 2.0540 (2.0540)  loss_giou_aux_2: 2.0606 (2.0606)  loss_giou_aux_3: 2.0731 (2.0731)  loss_giou_aux_4: 2.0635 (2.0635)  loss_giou_aux_5: 2.0868 (2.0868)  loss_giou_dn_0: 1.3425 (1.3425)  loss_giou_dn_1: 1.3425 (1.3425)  loss_giou_dn_2: 1.3425 (1.3425)  loss_giou_dn_3: 1.3425 (1.3425)  loss_giou_dn_4: 1.3425 (1.3425)  loss_giou_dn_5: 1.3425 (1.3425)  loss_vfl: 0.2448 (0.2448)  loss_vfl_aux_0: 0.2291 (0.2291)  loss_vfl_aux_1: 0.2344 (0.2344)  loss_vfl_aux_2: 0.2298 (0.2298)  loss_vfl_aux_3: 0.2321 (0.2321)  loss_vfl_aux_4: 0.2245 (0.2245)  loss_vfl_aux_5: 0.2566 (0.2566)  loss_vfl_dn_0: 0.8785 (0.8785)  loss_vfl_dn_1: 0.8373 (0.8373)  loss_vfl_dn_2: 0.8452 (0.8452)  loss_vfl_dn_3: 0.8350 (0.8350)  loss_vfl_dn_4: 0.8622 (0.8622)  loss_vfl_dn_5: 0.8596 (0.8596)  time: 1.6709  data: 0.4172  max mem: 4427
