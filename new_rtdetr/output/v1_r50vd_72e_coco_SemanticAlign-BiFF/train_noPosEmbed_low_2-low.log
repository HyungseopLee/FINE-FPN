WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Initialized distributed mode...
Initialized distributed mode...
cfg:  {'task': 'detection', '_model': None, '_postprocessor': None, '_criterion': None, '_optimizer': None, '_lr_scheduler': None, '_lr_warmup_scheduler': None, '_train_dataloader': None, '_val_dataloader': None, '_ema': None, '_scaler': None, '_train_dataset': None, '_val_dataset': None, '_collate_fn': None, '_evaluator': None, '_writer': None, 'num_workers': 0, 'batch_size': None, '_train_batch_size': None, '_val_batch_size': None, '_train_shuffle': None, '_val_shuffle': None, 'resume': None, 'tuning': None, 'epoches': 72, 'last_epoch': -1, 'use_amp': False, 'use_ema': True, 'ema_decay': 0.9999, 'ema_warmups': 2000, 'sync_bn': True, 'clip_max_norm': 0.1, 'find_unused_parameters': False, 'seed': None, 'print_freq': 100, 'checkpoint_freq': 20, 'output_dir': './output/v1_r50vd_72e_coco_SemanticAlign-BiFF', 'summary_dir': None, 'device': '', 'yaml_cfg': {'task': 'detection', 'evaluator': {'type': 'CocoEvaluator', 'iou_types': ['bbox']}, 'num_classes': 80, 'remap_mscoco_category': True, 'train_dataloader': {'type': 'DataLoader', 'dataset': {'type': 'CocoDetection', 'img_folder': '/media/data/coco/images/train2017', 'ann_file': '/media/data/coco/annotations/instances_train2017.json', 'return_masks': False, 'transforms': {'type': 'Compose', 'ops': [{'type': 'RandomPhotometricDistort', 'p': 0.5}, {'type': 'RandomZoomOut', 'fill': 0}, {'type': 'RandomIoUCrop', 'p': 0.8}, {'type': 'SanitizeBoundingBoxes', 'min_size': 1}, {'type': 'RandomHorizontalFlip'}, {'type': 'Resize', 'size': [640, 640]}, {'type': 'SanitizeBoundingBoxes', 'min_size': 1}, {'type': 'ConvertPILImage', 'dtype': 'float32', 'scale': True}, {'type': 'ConvertBoxes', 'fmt': 'cxcywh', 'normalize': True}]}}, 'shuffle': True, 'num_workers': 4, 'drop_last': True, 'collate_fn': {'type': 'BatchImageCollateFuncion', 'scales': [480, 512, 544, 576, 608, 640, 640, 640, 672, 704, 736, 768, 800]}, 'total_batch_size': 16}, 'val_dataloader': {'type': 'DataLoader', 'dataset': {'type': 'CocoDetection', 'img_folder': '/media/data/coco/images/val2017/', 'ann_file': '/media/data/coco/annotations/instances_val2017.json', 'return_masks': False, 'transforms': {'type': 'Compose', 'ops': [{'type': 'Resize', 'size': [640, 640]}, {'type': 'ConvertPILImage', 'dtype': 'float32', 'scale': True}]}}, 'shuffle': False, 'num_workers': 8, 'drop_last': False, 'collate_fn': {'type': 'BatchImageCollateFuncion'}, 'total_batch_size': 16}, 'print_freq': 100, 'output_dir': './output/v1_r50vd_72e_coco_SemanticAlign-BiFF', 'checkpoint_freq': 20, 'sync_bn': True, 'find_unused_parameters': False, 'use_amp': False, 'scaler': {'type': 'GradScaler', 'enabled': True}, 'use_ema': True, 'ema': {'type': 'ModelEMA', 'decay': 0.9999, 'warmups': 2000}, 'epoches': 72, 'clip_max_norm': 0.1, 'optimizer': {'type': 'AdamW', 'params': [{'params': '^(?=.*backbone)(?!.*(?:norm|bn)).*$', 'lr': 1e-05}, {'params': '^(?=.*backbone)(?=.*(?:norm|bn)).*$', 'weight_decay': 0.0, 'lr': 1e-05}, {'params': '^(?=.*(?:encoder|decoder))(?=.*(?:norm|bn|bias)).*$', 'weight_decay': 0.0}], 'lr': 0.0001, 'betas': [0.9, 0.999], 'weight_decay': 0.0001}, 'lr_scheduler': {'type': 'MultiStepLR', 'milestones': [1000], 'gamma': 0.1}, 'lr_warmup_scheduler': {'type': 'LinearWarmup', 'warmup_duration': 2000}, 'model': 'RTDETR', 'criterion': 'RTDETRCriterion', 'postprocessor': 'RTDETRPostProcessor', 'use_focal_loss': True, 'eval_spatial_size': [640, 640], 'RTDETR': {'backbone': 'PResNet', 'encoder': 'HybridEncoder', 'decoder': 'RTDETRTransformer'}, 'PResNet': {'depth': 50, 'variant': 'd', 'freeze_at': 0, 'return_idx': [1, 2, 3], 'num_stages': 4, 'freeze_norm': True, 'pretrained': True}, 'HybridEncoder': {'in_channels': [512, 1024, 2048], 'feat_strides': [8, 16, 32], 'hidden_dim': 256, 'use_encoder_idx': [2], 'num_encoder_layers': 1, 'nhead': 8, 'dim_feedforward': 1024, 'dropout': 0.0, 'enc_act': 'gelu', 'expansion': 1.0, 'depth_mult': 1, 'act': 'silu', 'version': 'v1'}, 'RTDETRTransformer': {'feat_channels': [256, 256, 256], 'feat_strides': [8, 16, 32], 'hidden_dim': 256, 'num_levels': 3, 'num_layers': 6, 'num_queries': 300, 'num_denoising': 100, 'label_noise_ratio': 0.5, 'box_noise_scale': 1.0, 'eval_idx': -1}, 'RTDETRPostProcessor': {'num_top_queries': 300}, 'RTDETRCriterion': {'weight_dict': {'loss_vfl': 1, 'loss_bbox': 5, 'loss_giou': 2}, 'losses': ['vfl', 'boxes'], 'alpha': 0.75, 'gamma': 2.0, 'matcher': {'type': 'HungarianMatcher', 'weight_dict': {'cost_class': 2, 'cost_bbox': 5, 'cost_giou': 2}, 'alpha': 0.25, 'gamma': 2.0}}, '__include__': ['../dataset/coco_detection.yml', '../runtime.yml', './include/dataloader.yml', './include/optimizer.yml', './include/rtdetr_r50vd.yml'], 'config': '/home/hslee/SONeck/rtdetrv2_pytorch2/configs/rtdetr/rtdetr_r50vd_6x_coco.yml', 'test_only': False, 'print_method': 'builtin', 'print_rank': 0}}
Start training
Load PResNet50 state_dict
model: RTDETR(
  (backbone): PResNet(
    (conv1): Sequential(
      (conv1_1): ConvNormLayer(
        (conv): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (norm): FrozenBatchNorm2d(32, eps=1e-05)
        (act): ReLU(inplace=True)
      )
      (conv1_2): ConvNormLayer(
        (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (norm): FrozenBatchNorm2d(32, eps=1e-05)
        (act): ReLU(inplace=True)
      )
      (conv1_3): ConvNormLayer(
        (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (norm): FrozenBatchNorm2d(64, eps=1e-05)
        (act): ReLU(inplace=True)
      )
    )
    (res_layers): ModuleList(
      (0): Blocks(
        (blocks): ModuleList(
          (0): BottleNeck(
            (branch2a): ConvNormLayer(
              (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (norm): FrozenBatchNorm2d(64, eps=1e-05)
              (act): ReLU(inplace=True)
            )
            (branch2b): ConvNormLayer(
              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm): FrozenBatchNorm2d(64, eps=1e-05)
              (act): ReLU(inplace=True)
            )
            (branch2c): ConvNormLayer(
              (conv): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (norm): FrozenBatchNorm2d(256, eps=1e-05)
              (act): Identity()
            )
            (short): ConvNormLayer(
              (conv): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (norm): FrozenBatchNorm2d(256, eps=1e-05)
              (act): Identity()
            )
            (act): ReLU(inplace=True)
          )
          (1-2): 2 x BottleNeck(
            (branch2a): ConvNormLayer(
              (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (norm): FrozenBatchNorm2d(64, eps=1e-05)
              (act): ReLU(inplace=True)
            )
            (branch2b): ConvNormLayer(
              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm): FrozenBatchNorm2d(64, eps=1e-05)
              (act): ReLU(inplace=True)
            )
            (branch2c): ConvNormLayer(
              (conv): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (norm): FrozenBatchNorm2d(256, eps=1e-05)
              (act): Identity()
            )
            (act): ReLU(inplace=True)
          )
        )
      )
      (1): Blocks(
        (blocks): ModuleList(
          (0): BottleNeck(
            (branch2a): ConvNormLayer(
              (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (norm): FrozenBatchNorm2d(128, eps=1e-05)
              (act): ReLU(inplace=True)
            )
            (branch2b): ConvNormLayer(
              (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
              (norm): FrozenBatchNorm2d(128, eps=1e-05)
              (act): ReLU(inplace=True)
            )
            (branch2c): ConvNormLayer(
              (conv): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (norm): FrozenBatchNorm2d(512, eps=1e-05)
              (act): Identity()
            )
            (short): Sequential(
              (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)
              (conv): ConvNormLayer(
                (conv): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (norm): FrozenBatchNorm2d(512, eps=1e-05)
                (act): Identity()
              )
            )
            (act): ReLU(inplace=True)
          )
          (1-3): 3 x BottleNeck(
            (branch2a): ConvNormLayer(
              (conv): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (norm): FrozenBatchNorm2d(128, eps=1e-05)
              (act): ReLU(inplace=True)
            )
            (branch2b): ConvNormLayer(
              (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm): FrozenBatchNorm2d(128, eps=1e-05)
              (act): ReLU(inplace=True)
            )
            (branch2c): ConvNormLayer(
              (conv): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (norm): FrozenBatchNorm2d(512, eps=1e-05)
              (act): Identity()
            )
            (act): ReLU(inplace=True)
          )
        )
      )
      (2): Blocks(
        (blocks): ModuleList(
          (0): BottleNeck(
            (branch2a): ConvNormLayer(
              (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (norm): FrozenBatchNorm2d(256, eps=1e-05)
              (act): ReLU(inplace=True)
            )
            (branch2b): ConvNormLayer(
              (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
              (norm): FrozenBatchNorm2d(256, eps=1e-05)
              (act): ReLU(inplace=True)
            )
            (branch2c): ConvNormLayer(
              (conv): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (norm): FrozenBatchNorm2d(1024, eps=1e-05)
              (act): Identity()
            )
            (short): Sequential(
              (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)
              (conv): ConvNormLayer(
                (conv): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (norm): FrozenBatchNorm2d(1024, eps=1e-05)
                (act): Identity()
              )
            )
            (act): ReLU(inplace=True)
          )
          (1-5): 5 x BottleNeck(
            (branch2a): ConvNormLayer(
              (conv): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (norm): FrozenBatchNorm2d(256, eps=1e-05)
              (act): ReLU(inplace=True)
            )
            (branch2b): ConvNormLayer(
              (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm): FrozenBatchNorm2d(256, eps=1e-05)
              (act): ReLU(inplace=True)
            )
            (branch2c): ConvNormLayer(
              (conv): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (norm): FrozenBatchNorm2d(1024, eps=1e-05)
              (act): Identity()
            )
            (act): ReLU(inplace=True)
          )
        )
      )
      (3): Blocks(
        (blocks): ModuleList(
          (0): BottleNeck(
            (branch2a): ConvNormLayer(
              (conv): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (norm): FrozenBatchNorm2d(512, eps=1e-05)
              (act): ReLU(inplace=True)
            )
            (branch2b): ConvNormLayer(
              (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
              (norm): FrozenBatchNorm2d(512, eps=1e-05)
              (act): ReLU(inplace=True)
            )
            (branch2c): ConvNormLayer(
              (conv): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (norm): FrozenBatchNorm2d(2048, eps=1e-05)
              (act): Identity()
            )
            (short): Sequential(
              (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)
              (conv): ConvNormLayer(
                (conv): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (norm): FrozenBatchNorm2d(2048, eps=1e-05)
                (act): Identity()
              )
            )
            (act): ReLU(inplace=True)
          )
          (1-2): 2 x BottleNeck(
            (branch2a): ConvNormLayer(
              (conv): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (norm): FrozenBatchNorm2d(512, eps=1e-05)
              (act): ReLU(inplace=True)
            )
            (branch2b): ConvNormLayer(
              (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm): FrozenBatchNorm2d(512, eps=1e-05)
              (act): ReLU(inplace=True)
            )
            (branch2c): ConvNormLayer(
              (conv): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (norm): FrozenBatchNorm2d(2048, eps=1e-05)
              (act): Identity()
            )
            (act): ReLU(inplace=True)
          )
        )
      )
    )
  )
  (decoder): RTDETRTransformer(
    (input_proj): ModuleList(
      (0-2): 3 x Sequential(
        (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (decoder): TransformerDecoder(
      (layers): ModuleList(
        (0-5): 6 x TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (dropout1): Dropout(p=0.0, inplace=False)
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (cross_attn): MSDeformableAttention(
            (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
            (attention_weights): Linear(in_features=256, out_features=96, bias=True)
            (value_proj): Linear(in_features=256, out_features=256, bias=True)
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
          (dropout2): Dropout(p=0.0, inplace=False)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (linear1): Linear(in_features=256, out_features=1024, bias=True)
          (dropout3): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout4): Dropout(p=0.0, inplace=False)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (denoising_class_embed): Embedding(81, 256, padding_idx=80)
    (query_pos_head): MLP(
      (layers): ModuleList(
        (0): Linear(in_features=4, out_features=512, bias=True)
        (1): Linear(in_features=512, out_features=256, bias=True)
      )
      (act): ReLU(inplace=True)
    )
    (enc_output): Sequential(
      (0): Linear(in_features=256, out_features=256, bias=True)
      (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
    (enc_score_head): Linear(in_features=256, out_features=80, bias=True)
    (enc_bbox_head): MLP(
      (layers): ModuleList(
        (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)
        (2): Linear(in_features=256, out_features=4, bias=True)
      )
      (act): ReLU(inplace=True)
    )
    (dec_score_head): ModuleList(
      (0-5): 6 x Linear(in_features=256, out_features=80, bias=True)
    )
    (dec_bbox_head): ModuleList(
      (0-5): 6 x MLP(
        (layers): ModuleList(
          (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
        (act): ReLU(inplace=True)
      )
    )
  )
  (encoder): HybridEncoder(
    (input_proj): ModuleList(
      (0): Sequential(
        (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): Sequential(
        (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): Sequential(
        (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (encoder): ModuleList(
      (0): TransformerEncoder(
        (layers): ModuleList(
          (0): TransformerEncoderLayer(
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout1): Dropout(p=0.0, inplace=False)
            (dropout2): Dropout(p=0.0, inplace=False)
            (activation): GELU(approximate='none')
          )
        )
      )
    )
    (lateral_convs): ModuleList(
      (0-1): 2 x ConvNormLayer(
        (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): SiLU(inplace=True)
      )
    )
    (fpn_blocks): ModuleList(
      (0-1): 2 x CSPRepLayer(
        (conv1): ConvNormLayer(
          (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act): SiLU(inplace=True)
        )
        (conv2): ConvNormLayer(
          (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act): SiLU(inplace=True)
        )
        (bottlenecks): Sequential(
          (0): RepVggBlock(
            (conv1): ConvNormLayer(
              (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act): Identity()
            )
            (conv2): ConvNormLayer(
              (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act): Identity()
            )
            (act): SiLU(inplace=True)
          )
          (1): RepVggBlock(
            (conv1): ConvNormLayer(
              (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act): Identity()
            )
            (conv2): ConvNormLayer(
              (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act): Identity()
            )
            (act): SiLU(inplace=True)
          )
          (2): RepVggBlock(
            (conv1): ConvNormLayer(
              (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act): Identity()
            )
            (conv2): ConvNormLayer(
              (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act): Identity()
            )
            (act): SiLU(inplace=True)
          )
        )
        (conv3): Identity()
      )
    )
    (downsample_convs): ModuleList(
      (0-1): 2 x ConvNormLayer(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): SiLU(inplace=True)
      )
    )
    (pan_blocks): ModuleList(
      (0-1): 2 x CSPRepLayer(
        (conv1): ConvNormLayer(
          (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act): SiLU(inplace=True)
        )
        (conv2): ConvNormLayer(
          (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act): SiLU(inplace=True)
        )
        (bottlenecks): Sequential(
          (0): RepVggBlock(
            (conv1): ConvNormLayer(
              (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act): Identity()
            )
            (conv2): ConvNormLayer(
              (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act): Identity()
            )
            (act): SiLU(inplace=True)
          )
          (1): RepVggBlock(
            (conv1): ConvNormLayer(
              (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act): Identity()
            )
            (conv2): ConvNormLayer(
              (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act): Identity()
            )
            (act): SiLU(inplace=True)
          )
          (2): RepVggBlock(
            (conv1): ConvNormLayer(
              (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act): Identity()
            )
            (conv2): ConvNormLayer(
              (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act): Identity()
            )
            (act): SiLU(inplace=True)
          )
        )
        (conv3): Identity()
      )
    )
    (sa_cross_attn): ModuleList(
      (0-1): 2 x SpatialAlignCrossAttention(
        (low_fusion_factor_head): Sequential(
          (0): Linear(in_features=256, out_features=64, bias=True)
          (1): ReLU()
          (2): Linear(in_features=64, out_features=1, bias=True)
        )
        (cross_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
        )
        (linear1): Linear(in_features=256, out_features=1024, bias=True)
        (dropout_layer): Dropout(p=0.0, inplace=False)
        (linear2): Linear(in_features=1024, out_features=256, bias=True)
        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (activation): GELU(approximate='none')
        (avg_pool_2x): AvgPool2d(kernel_size=2, stride=2, padding=0)
        (avg_pool_4x): AvgPool2d(kernel_size=4, stride=4, padding=0)
      )
    )
  )
)
| module                                  | #parameters or shape   | #flops     |
|:----------------------------------------|:-----------------------|:-----------|
| module                                  | 44.504M                | 69.796G    |
|  backbone                               |  23.474M               |  35.321G   |
|   backbone.conv1                        |   28.512K              |   2.92G    |
|    backbone.conv1.conv1_1.conv          |    0.864K              |    88.474M |
|    backbone.conv1.conv1_2.conv          |    9.216K              |    0.944G  |
|    backbone.conv1.conv1_3.conv          |    18.432K             |    1.887G  |
|   backbone.res_layers                   |   23.446M              |   32.401G  |
|    backbone.res_layers.0.blocks         |    0.213M              |    5.453G  |
|    backbone.res_layers.1.blocks         |    1.212M              |    8.389G  |
|    backbone.res_layers.2.blocks         |    7.078M              |    11.954G |
|    backbone.res_layers.3.blocks         |    14.942M             |    6.606G  |
|  decoder                                |  7.467M                |  8.156G    |
|   decoder.input_proj                    |   0.198M               |   0.555G   |
|    decoder.input_proj.0                 |    66.048K             |    0.423G  |
|    decoder.input_proj.1                 |    66.048K             |    0.106G  |
|    decoder.input_proj.2                 |    66.048K             |    26.419M |
|   decoder.decoder.layers                |   5.975M               |   5.275G   |
|    decoder.decoder.layers.0             |    0.996M              |    0.879G  |
|    decoder.decoder.layers.1             |    0.996M              |    0.879G  |
|    decoder.decoder.layers.2             |    0.996M              |    0.879G  |
|    decoder.decoder.layers.3             |    0.996M              |    0.879G  |
|    decoder.decoder.layers.4             |    0.996M              |    0.879G  |
|    decoder.decoder.layers.5             |    0.996M              |    0.879G  |
|   decoder.denoising_class_embed         |   20.736K              |            |
|    decoder.denoising_class_embed.weight |    (81, 256)           |            |
|   decoder.query_pos_head.layers         |   0.134M               |   0.24G    |
|    decoder.query_pos_head.layers.0      |    2.56K               |    3.686M  |
|    decoder.query_pos_head.layers.1      |    0.131M              |    0.236G  |
|   decoder.enc_output                    |   66.304K              |   0.561G   |
|    decoder.enc_output.0                 |    65.792K             |    0.551G  |
|    decoder.enc_output.1                 |    0.512K              |    10.752M |
|   decoder.enc_score_head                |   20.56K               |   0.172G   |
|    decoder.enc_score_head.weight        |    (80, 256)           |            |
|    decoder.enc_score_head.bias          |    (80,)               |            |
|   decoder.enc_bbox_head.layers          |   0.133M               |   1.11G    |
|    decoder.enc_bbox_head.layers.0       |    65.792K             |    0.551G  |
|    decoder.enc_bbox_head.layers.1       |    65.792K             |    0.551G  |
|    decoder.enc_bbox_head.layers.2       |    1.028K              |    8.602M  |
|   decoder.dec_score_head                |   0.123M               |   6.144M   |
|    decoder.dec_score_head.0             |    20.56K              |            |
|    decoder.dec_score_head.1             |    20.56K              |            |
|    decoder.dec_score_head.2             |    20.56K              |            |
|    decoder.dec_score_head.3             |    20.56K              |            |
|    decoder.dec_score_head.4             |    20.56K              |            |
|    decoder.dec_score_head.5             |    20.56K              |    6.144M  |
|   decoder.dec_bbox_head                 |   0.796M               |   0.238G   |
|    decoder.dec_bbox_head.0.layers       |    0.133M              |    39.629M |
|    decoder.dec_bbox_head.1.layers       |    0.133M              |    39.629M |
|    decoder.dec_bbox_head.2.layers       |    0.133M              |    39.629M |
|    decoder.dec_bbox_head.3.layers       |    0.133M              |    39.629M |
|    decoder.dec_bbox_head.4.layers       |    0.133M              |    39.629M |
|    decoder.dec_bbox_head.5.layers       |    0.133M              |    39.629M |
|  encoder                                |  13.564M               |  26.32G    |
|   encoder.input_proj                    |   0.919M               |   1.472G   |
|    encoder.input_proj.0                 |    0.132M              |    0.842G  |
|    encoder.input_proj.1                 |    0.263M              |    0.42G   |
|    encoder.input_proj.2                 |    0.525M              |    0.21G   |
|   encoder.encoder.0.layers.0            |   0.79M                |   0.398G   |
|    encoder.encoder.0.layers.0.self_attn |    0.263M              |    0.187G  |
|    encoder.encoder.0.layers.0.linear1   |    0.263M              |    0.105G  |
|    encoder.encoder.0.layers.0.linear2   |    0.262M              |    0.105G  |
|    encoder.encoder.0.layers.0.norm1     |    0.512K              |    0.512M  |
|    encoder.encoder.0.layers.0.norm2     |    0.512K              |    0.512M  |
|   encoder.lateral_convs                 |   0.132M               |   0.132G   |
|    encoder.lateral_convs.0              |    66.048K             |    26.419M |
|    encoder.lateral_convs.1              |    66.048K             |    0.106G  |
|   encoder.fpn_blocks                    |   4.465M               |   17.859G  |
|    encoder.fpn_blocks.0                 |    2.232M              |    3.572G  |
|    encoder.fpn_blocks.1                 |    2.232M              |    14.287G |
|   encoder.downsample_convs              |   1.181M               |   1.181G   |
|    encoder.downsample_convs.0           |    0.59M               |    0.945G  |
|    encoder.downsample_convs.1           |    0.59M               |    0.236G  |
|   encoder.pan_blocks                    |   4.465M               |   4.465G   |
|    encoder.pan_blocks.0                 |    2.232M              |    3.572G  |
|    encoder.pan_blocks.1                 |    2.232M              |    0.893G  |
|   encoder.sa_cross_attn                 |   1.613M               |   0.806G   |
|    encoder.sa_cross_attn.0              |    0.807M              |    0.4G    |
|    encoder.sa_cross_attn.1              |    0.807M              |    0.405G  |
Initial lr: [1e-05, 1e-05, 0.0001, 0.0001]
building train_dataloader with batch_size=8...
loading annotations into memory...
Done (t=8.18s)
creating index...
index created!
building val_dataloader with batch_size=8...
loading annotations into memory...
Done (t=0.21s)
creating index...
index created!
number of trainable parameters: 44475918
Epoch: [0]  [   0/7393]  eta: 2:24:22  lr: 0.000000  loss: 48.3778 (48.3778)  loss_bbox: 1.9755 (1.9755)  loss_bbox_aux_0: 2.0087 (2.0087)  loss_bbox_aux_1: 1.9586 (1.9586)  loss_bbox_aux_2: 1.9802 (1.9802)  loss_bbox_aux_3: 1.9701 (1.9701)  loss_bbox_aux_4: 1.9557 (1.9557)  loss_bbox_aux_5: 2.0234 (2.0234)  loss_bbox_dn_0: 1.1570 (1.1570)  loss_bbox_dn_1: 1.1570 (1.1570)  loss_bbox_dn_2: 1.1570 (1.1570)  loss_bbox_dn_3: 1.1570 (1.1570)  loss_bbox_dn_4: 1.1570 (1.1570)  loss_bbox_dn_5: 1.1570 (1.1570)  loss_giou: 1.7972 (1.7972)  loss_giou_aux_0: 1.8228 (1.8228)  loss_giou_aux_1: 1.8055 (1.8055)  loss_giou_aux_2: 1.8147 (1.8147)  loss_giou_aux_3: 1.8143 (1.8143)  loss_giou_aux_4: 1.8068 (1.8068)  loss_giou_aux_5: 1.8216 (1.8216)  loss_giou_dn_0: 1.3205 (1.3205)  loss_giou_dn_1: 1.3205 (1.3205)  loss_giou_dn_2: 1.3205 (1.3205)  loss_giou_dn_3: 1.3205 (1.3205)  loss_giou_dn_4: 1.3205 (1.3205)  loss_giou_dn_5: 1.3205 (1.3205)  loss_vfl: 0.2704 (0.2704)  loss_vfl_aux_0: 0.2364 (0.2364)  loss_vfl_aux_1: 0.2666 (0.2666)  loss_vfl_aux_2: 0.2687 (0.2687)  loss_vfl_aux_3: 0.2514 (0.2514)  loss_vfl_aux_4: 0.2365 (0.2365)  loss_vfl_aux_5: 0.2579 (0.2579)  loss_vfl_dn_0: 0.8618 (0.8618)  loss_vfl_dn_1: 0.8398 (0.8398)  loss_vfl_dn_2: 0.8696 (0.8696)  loss_vfl_dn_3: 0.8691 (0.8691)  loss_vfl_dn_4: 0.8337 (0.8337)  loss_vfl_dn_5: 0.8958 (0.8958)  time: 1.1717  data: 0.7281  max mem: 8484
Epoch: [0]  [ 100/7393]  eta: 0:43:15  lr: 0.000001  loss: 43.3749 (44.5895)  loss_bbox: 1.5023 (1.6027)  loss_bbox_aux_0: 1.4933 (1.6188)  loss_bbox_aux_1: 1.5246 (1.6149)  loss_bbox_aux_2: 1.4970 (1.6058)  loss_bbox_aux_3: 1.4999 (1.6057)  loss_bbox_aux_4: 1.4894 (1.6016)  loss_bbox_aux_5: 1.5305 (1.6315)  loss_bbox_dn_0: 0.7758 (0.8463)  loss_bbox_dn_1: 0.7760 (0.8464)  loss_bbox_dn_2: 0.7763 (0.8466)  loss_bbox_dn_3: 0.7766 (0.8467)  loss_bbox_dn_4: 0.7769 (0.8468)  loss_bbox_dn_5: 0.7773 (0.8470)  loss_giou: 1.8922 (1.9652)  loss_giou_aux_0: 1.9637 (1.9861)  loss_giou_aux_1: 1.9171 (1.9766)  loss_giou_aux_2: 1.8984 (1.9742)  loss_giou_aux_3: 1.9022 (1.9678)  loss_giou_aux_4: 1.9061 (1.9708)  loss_giou_aux_5: 1.9443 (1.9954)  loss_giou_dn_0: 1.3289 (1.3372)  loss_giou_dn_1: 1.3288 (1.3370)  loss_giou_dn_2: 1.3287 (1.3369)  loss_giou_dn_3: 1.3285 (1.3367)  loss_giou_dn_4: 1.3282 (1.3366)  loss_giou_dn_5: 1.3279 (1.3365)  loss_vfl: 0.2008 (0.2146)  loss_vfl_aux_0: 0.1943 (0.2072)  loss_vfl_aux_1: 0.2022 (0.2078)  loss_vfl_aux_2: 0.2201 (0.2144)  loss_vfl_aux_3: 0.1993 (0.2120)  loss_vfl_aux_4: 0.2242 (0.2146)  loss_vfl_aux_5: 0.2240 (0.2185)  loss_vfl_dn_0: 0.8096 (0.8253)  loss_vfl_dn_1: 0.7530 (0.7996)  loss_vfl_dn_2: 0.8017 (0.8280)  loss_vfl_dn_3: 0.7690 (0.8210)  loss_vfl_dn_4: 0.7280 (0.7947)  loss_vfl_dn_5: 0.7482 (0.8137)  time: 0.3133  data: 0.0067  max mem: 12434
Epoch: [0]  [ 200/7393]  eta: 0:40:12  lr: 0.000001  loss: 41.2955 (43.4438)  loss_bbox: 1.3693 (1.5268)  loss_bbox_aux_0: 1.4065 (1.5588)  loss_bbox_aux_1: 1.4030 (1.5503)  loss_bbox_aux_2: 1.3854 (1.5392)  loss_bbox_aux_3: 1.3827 (1.5352)  loss_bbox_aux_4: 1.3792 (1.5292)  loss_bbox_aux_5: 1.4409 (1.5768)  loss_bbox_dn_0: 0.7794 (0.8436)  loss_bbox_dn_1: 0.7849 (0.8445)  loss_bbox_dn_2: 0.7937 (0.8460)  loss_bbox_dn_3: 0.8019 (0.8475)  loss_bbox_dn_4: 0.8124 (0.8495)  loss_bbox_dn_5: 0.8229 (0.8516)  loss_giou: 1.6685 (1.8697)  loss_giou_aux_0: 1.7365 (1.9071)  loss_giou_aux_1: 1.7140 (1.8946)  loss_giou_aux_2: 1.7071 (1.8874)  loss_giou_aux_3: 1.6870 (1.8786)  loss_giou_aux_4: 1.6802 (1.8761)  loss_giou_aux_5: 1.7752 (1.9254)  loss_giou_dn_0: 1.3351 (1.3386)  loss_giou_dn_1: 1.3340 (1.3379)  loss_giou_dn_2: 1.3343 (1.3375)  loss_giou_dn_3: 1.3333 (1.3372)  loss_giou_dn_4: 1.3328 (1.3371)  loss_giou_dn_5: 1.3344 (1.3372)  loss_vfl: 0.3626 (0.2612)  loss_vfl_aux_0: 0.2848 (0.2370)  loss_vfl_aux_1: 0.3014 (0.2403)  loss_vfl_aux_2: 0.3332 (0.2530)  loss_vfl_aux_3: 0.3374 (0.2553)  loss_vfl_aux_4: 0.3483 (0.2638)  loss_vfl_aux_5: 0.2828 (0.2473)  loss_vfl_dn_0: 0.6797 (0.7762)  loss_vfl_dn_1: 0.6608 (0.7409)  loss_vfl_dn_2: 0.6674 (0.7666)  loss_vfl_dn_3: 0.6460 (0.7547)  loss_vfl_dn_4: 0.6584 (0.7389)  loss_vfl_dn_5: 0.6342 (0.7452)  time: 0.3183  data: 0.0063  max mem: 12445
Epoch: [0]  [ 300/7393]  eta: 0:39:06  lr: 0.000002  loss: 38.4049 (42.4208)  loss_bbox: 1.1806 (1.4507)  loss_bbox_aux_0: 1.2317 (1.4971)  loss_bbox_aux_1: 1.2251 (1.4821)  loss_bbox_aux_2: 1.2162 (1.4686)  loss_bbox_aux_3: 1.1886 (1.4619)  loss_bbox_aux_4: 1.1916 (1.4549)  loss_bbox_aux_5: 1.2314 (1.5225)  loss_bbox_dn_0: 0.7430 (0.8319)  loss_bbox_dn_1: 0.7657 (0.8367)  loss_bbox_dn_2: 0.7860 (0.8427)  loss_bbox_dn_3: 0.7994 (0.8478)  loss_bbox_dn_4: 0.8104 (0.8532)  loss_bbox_dn_5: 0.8199 (0.8586)  loss_giou: 1.5095 (1.7722)  loss_giou_aux_0: 1.5613 (1.8223)  loss_giou_aux_1: 1.5378 (1.8050)  loss_giou_aux_2: 1.5261 (1.7937)  loss_giou_aux_3: 1.5227 (1.7837)  loss_giou_aux_4: 1.5143 (1.7791)  loss_giou_aux_5: 1.6009 (1.8486)  loss_giou_dn_0: 1.3366 (1.3386)  loss_giou_dn_1: 1.3390 (1.3379)  loss_giou_dn_2: 1.3428 (1.3384)  loss_giou_dn_3: 1.3481 (1.3393)  loss_giou_dn_4: 1.3515 (1.3405)  loss_giou_dn_5: 1.3559 (1.3423)  loss_vfl: 0.4878 (0.3193)  loss_vfl_aux_0: 0.4203 (0.2839)  loss_vfl_aux_1: 0.4311 (0.2900)  loss_vfl_aux_2: 0.4576 (0.3073)  loss_vfl_aux_3: 0.4697 (0.3118)  loss_vfl_aux_4: 0.4693 (0.3194)  loss_vfl_aux_5: 0.4002 (0.2811)  loss_vfl_dn_0: 0.6380 (0.7353)  loss_vfl_dn_1: 0.5983 (0.7025)  loss_vfl_dn_2: 0.5955 (0.7201)  loss_vfl_dn_3: 0.5826 (0.7072)  loss_vfl_dn_4: 0.5910 (0.6964)  loss_vfl_dn_5: 0.5891 (0.6967)  time: 0.3406  data: 0.0066  max mem: 12445
Epoch: [0]  [ 400/7393]  eta: 0:38:11  lr: 0.000002  loss: 37.7543 (41.6530)  loss_bbox: 0.9389 (1.3729)  loss_bbox_aux_0: 0.9757 (1.4243)  loss_bbox_aux_1: 0.9666 (1.4059)  loss_bbox_aux_2: 0.9496 (1.3918)  loss_bbox_aux_3: 0.9473 (1.3847)  loss_bbox_aux_4: 0.9468 (1.3779)  loss_bbox_aux_5: 1.0255 (1.4566)  loss_bbox_dn_0: 0.7938 (0.8426)  loss_bbox_dn_1: 0.8053 (0.8490)  loss_bbox_dn_2: 0.8091 (0.8553)  loss_bbox_dn_3: 0.8090 (0.8601)  loss_bbox_dn_4: 0.8047 (0.8643)  loss_bbox_dn_5: 0.8003 (0.8684)  loss_giou: 1.3695 (1.6883)  loss_giou_aux_0: 1.3916 (1.7358)  loss_giou_aux_1: 1.3757 (1.7183)  loss_giou_aux_2: 1.3709 (1.7073)  loss_giou_aux_3: 1.3703 (1.6984)  loss_giou_aux_4: 1.3666 (1.6941)  loss_giou_aux_5: 1.4126 (1.7634)  loss_giou_dn_0: 1.3333 (1.3371)  loss_giou_dn_1: 1.3283 (1.3358)  loss_giou_dn_2: 1.3229 (1.3360)  loss_giou_dn_3: 1.3206 (1.3366)  loss_giou_dn_4: 1.3160 (1.3375)  loss_giou_dn_5: 1.3143 (1.3393)  loss_vfl: 0.6748 (0.3908)  loss_vfl_aux_0: 0.6416 (0.3512)  loss_vfl_aux_1: 0.6478 (0.3591)  loss_vfl_aux_2: 0.6622 (0.3762)  loss_vfl_aux_3: 0.6788 (0.3831)  loss_vfl_aux_4: 0.6800 (0.3898)  loss_vfl_aux_5: 0.5923 (0.3381)  loss_vfl_dn_0: 0.5867 (0.7047)  loss_vfl_dn_1: 0.5761 (0.6743)  loss_vfl_dn_2: 0.5715 (0.6876)  loss_vfl_dn_3: 0.5802 (0.6762)  loss_vfl_dn_4: 0.5905 (0.6700)  loss_vfl_dn_5: 0.5932 (0.6702)  time: 0.3402  data: 0.0064  max mem: 12445
Epoch: [0]  [ 500/7393]  eta: 0:37:21  lr: 0.000003  loss: 36.0780 (40.8227)  loss_bbox: 0.7858 (1.2739)  loss_bbox_aux_0: 0.8621 (1.3312)  loss_bbox_aux_1: 0.8356 (1.3123)  loss_bbox_aux_2: 0.8253 (1.2972)  loss_bbox_aux_3: 0.8223 (1.2882)  loss_bbox_aux_4: 0.7880 (1.2800)  loss_bbox_aux_5: 0.8808 (1.3653)  loss_bbox_dn_0: 0.7902 (0.8432)  loss_bbox_dn_1: 0.7834 (0.8483)  loss_bbox_dn_2: 0.7704 (0.8518)  loss_bbox_dn_3: 0.7611 (0.8546)  loss_bbox_dn_4: 0.7555 (0.8570)  loss_bbox_dn_5: 0.7533 (0.8597)  loss_giou: 1.3515 (1.6279)  loss_giou_aux_0: 1.4358 (1.6763)  loss_giou_aux_1: 1.4143 (1.6594)  loss_giou_aux_2: 1.3937 (1.6481)  loss_giou_aux_3: 1.3931 (1.6389)  loss_giou_aux_4: 1.3668 (1.6335)  loss_giou_aux_5: 1.4602 (1.7044)  loss_giou_dn_0: 1.3344 (1.3351)  loss_giou_dn_1: 1.3270 (1.3326)  loss_giou_dn_2: 1.3193 (1.3323)  loss_giou_dn_3: 1.3300 (1.3331)  loss_giou_dn_4: 1.3366 (1.3352)  loss_giou_dn_5: 1.3469 (1.3383)  loss_vfl: 0.7282 (0.4566)  loss_vfl_aux_0: 0.6133 (0.4132)  loss_vfl_aux_1: 0.6344 (0.4213)  loss_vfl_aux_2: 0.6559 (0.4380)  loss_vfl_aux_3: 0.7017 (0.4463)  loss_vfl_aux_4: 0.7274 (0.4547)  loss_vfl_aux_5: 0.6095 (0.3969)  loss_vfl_dn_0: 0.5437 (0.6798)  loss_vfl_dn_1: 0.5284 (0.6515)  loss_vfl_dn_2: 0.5294 (0.6617)  loss_vfl_dn_3: 0.5293 (0.6518)  loss_vfl_dn_4: 0.5223 (0.6467)  loss_vfl_dn_5: 0.5093 (0.6463)  time: 0.3008  data: 0.0063  max mem: 12445
Epoch: [0]  [ 600/7393]  eta: 0:36:48  lr: 0.000003  loss: 34.6931 (40.0142)  loss_bbox: 0.5837 (1.1764)  loss_bbox_aux_0: 0.7246 (1.2432)  loss_bbox_aux_1: 0.6806 (1.2221)  loss_bbox_aux_2: 0.6130 (1.2031)  loss_bbox_aux_3: 0.5979 (1.1914)  loss_bbox_aux_4: 0.5873 (1.1823)  loss_bbox_aux_5: 0.7707 (1.2788)  loss_bbox_dn_0: 0.7398 (0.8416)  loss_bbox_dn_1: 0.7291 (0.8438)  loss_bbox_dn_2: 0.7178 (0.8442)  loss_bbox_dn_3: 0.7132 (0.8457)  loss_bbox_dn_4: 0.7113 (0.8472)  loss_bbox_dn_5: 0.7112 (0.8493)  loss_giou: 1.1518 (1.5605)  loss_giou_aux_0: 1.2844 (1.6179)  loss_giou_aux_1: 1.2265 (1.5987)  loss_giou_aux_2: 1.1919 (1.5833)  loss_giou_aux_3: 1.1678 (1.5720)  loss_giou_aux_4: 1.1514 (1.5660)  loss_giou_aux_5: 1.3295 (1.6475)  loss_giou_dn_0: 1.3153 (1.3332)  loss_giou_dn_1: 1.3011 (1.3289)  loss_giou_dn_2: 1.3038 (1.3284)  loss_giou_dn_3: 1.3054 (1.3301)  loss_giou_dn_4: 1.3075 (1.3334)  loss_giou_dn_5: 1.3095 (1.3371)  loss_vfl: 0.9168 (0.5292)  loss_vfl_aux_0: 0.7690 (0.4736)  loss_vfl_aux_1: 0.8130 (0.4849)  loss_vfl_aux_2: 0.8727 (0.5061)  loss_vfl_aux_3: 0.9086 (0.5176)  loss_vfl_aux_4: 0.9140 (0.5278)  loss_vfl_aux_5: 0.7147 (0.4532)  loss_vfl_dn_0: 0.5198 (0.6570)  loss_vfl_dn_1: 0.5192 (0.6326)  loss_vfl_dn_2: 0.5181 (0.6408)  loss_vfl_dn_3: 0.5168 (0.6316)  loss_vfl_dn_4: 0.5095 (0.6274)  loss_vfl_dn_5: 0.5130 (0.6264)  time: 0.3315  data: 0.0064  max mem: 12450
Epoch: [0]  [ 700/7393]  eta: 0:36:13  lr: 0.000004  loss: 33.3155 (39.3079)  loss_bbox: 0.5313 (1.0974)  loss_bbox_aux_0: 0.6014 (1.1681)  loss_bbox_aux_1: 0.5652 (1.1432)  loss_bbox_aux_2: 0.5422 (1.1229)  loss_bbox_aux_3: 0.5383 (1.1112)  loss_bbox_aux_4: 0.5298 (1.1027)  loss_bbox_aux_5: 0.7021 (1.2112)  loss_bbox_dn_0: 0.7100 (0.8386)  loss_bbox_dn_1: 0.6652 (0.8368)  loss_bbox_dn_2: 0.6425 (0.8346)  loss_bbox_dn_3: 0.6361 (0.8353)  loss_bbox_dn_4: 0.6355 (0.8363)  loss_bbox_dn_5: 0.6349 (0.8380)  loss_giou: 1.1233 (1.5011)  loss_giou_aux_0: 1.2110 (1.5630)  loss_giou_aux_1: 1.1561 (1.5394)  loss_giou_aux_2: 1.1362 (1.5227)  loss_giou_aux_3: 1.1239 (1.5119)  loss_giou_aux_4: 1.1312 (1.5061)  loss_giou_aux_5: 1.3299 (1.6005)  loss_giou_dn_0: 1.2964 (1.3279)  loss_giou_dn_1: 1.2617 (1.3206)  loss_giou_dn_2: 1.2534 (1.3192)  loss_giou_dn_3: 1.2514 (1.3207)  loss_giou_dn_4: 1.2446 (1.3236)  loss_giou_dn_5: 1.2425 (1.3271)  loss_vfl: 0.9610 (0.5939)  loss_vfl_aux_0: 0.8807 (0.5312)  loss_vfl_aux_1: 0.9133 (0.5474)  loss_vfl_aux_2: 0.9430 (0.5699)  loss_vfl_aux_3: 0.9634 (0.5816)  loss_vfl_aux_4: 0.9543 (0.5921)  loss_vfl_aux_5: 0.7069 (0.5006)  loss_vfl_dn_0: 0.5198 (0.6388)  loss_vfl_dn_1: 0.5257 (0.6186)  loss_vfl_dn_2: 0.5268 (0.6262)  loss_vfl_dn_3: 0.5227 (0.6182)  loss_vfl_dn_4: 0.5318 (0.6154)  loss_vfl_dn_5: 0.5259 (0.6139)  time: 0.3197  data: 0.0065  max mem: 12450
Epoch: [0]  [ 800/7393]  eta: 0:35:36  lr: 0.000004  loss: 33.8571 (38.6992)  loss_bbox: 0.5638 (1.0337)  loss_bbox_aux_0: 0.6162 (1.1036)  loss_bbox_aux_1: 0.5930 (1.0770)  loss_bbox_aux_2: 0.5821 (1.0574)  loss_bbox_aux_3: 0.5792 (1.0464)  loss_bbox_aux_4: 0.5667 (1.0387)  loss_bbox_aux_5: 0.7193 (1.1561)  loss_bbox_dn_0: 0.7767 (0.8332)  loss_bbox_dn_1: 0.7499 (0.8280)  loss_bbox_dn_2: 0.7436 (0.8245)  loss_bbox_dn_3: 0.7424 (0.8247)  loss_bbox_dn_4: 0.7435 (0.8254)  loss_bbox_dn_5: 0.7439 (0.8268)  loss_giou: 1.0396 (1.4535)  loss_giou_aux_0: 1.0942 (1.5155)  loss_giou_aux_1: 1.0688 (1.4907)  loss_giou_aux_2: 1.0642 (1.4742)  loss_giou_aux_3: 1.0313 (1.4638)  loss_giou_aux_4: 1.0447 (1.4582)  loss_giou_aux_5: 1.2304 (1.5633)  loss_giou_dn_0: 1.2504 (1.3204)  loss_giou_dn_1: 1.2079 (1.3107)  loss_giou_dn_2: 1.1900 (1.3083)  loss_giou_dn_3: 1.1908 (1.3095)  loss_giou_dn_4: 1.1885 (1.3121)  loss_giou_dn_5: 1.1909 (1.3154)  loss_vfl: 1.0688 (0.6457)  loss_vfl_aux_0: 0.9920 (0.5811)  loss_vfl_aux_1: 1.0014 (0.5990)  loss_vfl_aux_2: 1.0399 (0.6210)  loss_vfl_aux_3: 1.0170 (0.6330)  loss_vfl_aux_4: 1.0446 (0.6438)  loss_vfl_aux_5: 0.8381 (0.5367)  loss_vfl_dn_0: 0.5241 (0.6241)  loss_vfl_dn_1: 0.5390 (0.6078)  loss_vfl_dn_2: 0.5574 (0.6157)  loss_vfl_dn_3: 0.5515 (0.6085)  loss_vfl_dn_4: 0.5520 (0.6067)  loss_vfl_dn_5: 0.5576 (0.6048)  time: 0.3279  data: 0.0064  max mem: 12450
Epoch: [0]  [ 900/7393]  eta: 0:35:11  lr: 0.000005  loss: 33.3644 (38.1498)  loss_bbox: 0.5425 (0.9789)  loss_bbox_aux_0: 0.5666 (1.0463)  loss_bbox_aux_1: 0.5602 (1.0199)  loss_bbox_aux_2: 0.5520 (1.0012)  loss_bbox_aux_3: 0.5436 (0.9909)  loss_bbox_aux_4: 0.5387 (0.9836)  loss_bbox_aux_5: 0.6382 (1.1038)  loss_bbox_dn_0: 0.7198 (0.8251)  loss_bbox_dn_1: 0.6861 (0.8170)  loss_bbox_dn_2: 0.6837 (0.8126)  loss_bbox_dn_3: 0.6802 (0.8124)  loss_bbox_dn_4: 0.6778 (0.8128)  loss_bbox_dn_5: 0.6776 (0.8140)  loss_giou: 1.1047 (1.4120)  loss_giou_aux_0: 1.1702 (1.4728)  loss_giou_aux_1: 1.1388 (1.4483)  loss_giou_aux_2: 1.1295 (1.4318)  loss_giou_aux_3: 1.1268 (1.4218)  loss_giou_aux_4: 1.1038 (1.4164)  loss_giou_aux_5: 1.2554 (1.5273)  loss_giou_dn_0: 1.2227 (1.3111)  loss_giou_dn_1: 1.2031 (1.2991)  loss_giou_dn_2: 1.1856 (1.2955)  loss_giou_dn_3: 1.1788 (1.2963)  loss_giou_dn_4: 1.1767 (1.2987)  loss_giou_dn_5: 1.1785 (1.3019)  loss_vfl: 1.0554 (0.6930)  loss_vfl_aux_0: 0.9750 (0.6270)  loss_vfl_aux_1: 0.9974 (0.6447)  loss_vfl_aux_2: 1.0110 (0.6662)  loss_vfl_aux_3: 1.0179 (0.6789)  loss_vfl_aux_4: 1.0433 (0.6900)  loss_vfl_aux_5: 0.8775 (0.5745)  loss_vfl_dn_0: 0.5079 (0.6128)  loss_vfl_dn_1: 0.5317 (0.5999)  loss_vfl_dn_2: 0.5545 (0.6086)  loss_vfl_dn_3: 0.5403 (0.6022)  loss_vfl_dn_4: 0.5587 (0.6011)  loss_vfl_dn_5: 0.5543 (0.5992)  time: 0.3303  data: 0.0065  max mem: 12450
Epoch: [0]  [1000/7393]  eta: 0:34:39  lr: 0.000005  loss: 32.0375 (37.6735)  loss_bbox: 0.4499 (0.9327)  loss_bbox_aux_0: 0.5027 (0.9984)  loss_bbox_aux_1: 0.4683 (0.9720)  loss_bbox_aux_2: 0.4671 (0.9540)  loss_bbox_aux_3: 0.4610 (0.9439)  loss_bbox_aux_4: 0.4510 (0.9372)  loss_bbox_aux_5: 0.5904 (1.0595)  loss_bbox_dn_0: 0.6436 (0.8166)  loss_bbox_dn_1: 0.6073 (0.8058)  loss_bbox_dn_2: 0.5960 (0.8006)  loss_bbox_dn_3: 0.5913 (0.8000)  loss_bbox_dn_4: 0.5907 (0.8001)  loss_bbox_dn_5: 0.5900 (0.8012)  loss_giou: 1.0411 (1.3762)  loss_giou_aux_0: 1.0855 (1.4372)  loss_giou_aux_1: 1.0606 (1.4121)  loss_giou_aux_2: 1.0555 (1.3954)  loss_giou_aux_3: 1.0427 (1.3857)  loss_giou_aux_4: 1.0471 (1.3803)  loss_giou_aux_5: 1.2108 (1.4964)  loss_giou_dn_0: 1.2113 (1.3022)  loss_giou_dn_1: 1.1735 (1.2882)  loss_giou_dn_2: 1.1606 (1.2836)  loss_giou_dn_3: 1.1662 (1.2840)  loss_giou_dn_4: 1.1608 (1.2862)  loss_giou_dn_5: 1.1670 (1.2893)  loss_vfl: 1.0742 (0.7342)  loss_vfl_aux_0: 0.9467 (0.6650)  loss_vfl_aux_1: 0.9687 (0.6834)  loss_vfl_aux_2: 1.0040 (0.7057)  loss_vfl_aux_3: 1.0189 (0.7184)  loss_vfl_aux_4: 1.0394 (0.7301)  loss_vfl_aux_5: 0.8433 (0.6085)  loss_vfl_dn_0: 0.4912 (0.6032)  loss_vfl_dn_1: 0.5069 (0.5935)  loss_vfl_dn_2: 0.5275 (0.6031)  loss_vfl_dn_3: 0.5194 (0.5973)  loss_vfl_dn_4: 0.5325 (0.5970)  loss_vfl_dn_5: 0.5328 (0.5950)  time: 0.3211  data: 0.0064  max mem: 12450
Epoch: [0]  [1100/7393]  eta: 0:34:02  lr: 0.000006  loss: 32.6056 (37.2542)  loss_bbox: 0.4865 (0.8928)  loss_bbox_aux_0: 0.5343 (0.9578)  loss_bbox_aux_1: 0.5102 (0.9309)  loss_bbox_aux_2: 0.4986 (0.9134)  loss_bbox_aux_3: 0.4867 (0.9036)  loss_bbox_aux_4: 0.4864 (0.8970)  loss_bbox_aux_5: 0.5982 (1.0221)  loss_bbox_dn_0: 0.6869 (0.8101)  loss_bbox_dn_1: 0.6556 (0.7969)  loss_bbox_dn_2: 0.6483 (0.7908)  loss_bbox_dn_3: 0.6435 (0.7898)  loss_bbox_dn_4: 0.6440 (0.7898)  loss_bbox_dn_5: 0.6450 (0.7908)  loss_giou: 1.0380 (1.3432)  loss_giou_aux_0: 1.1092 (1.4051)  loss_giou_aux_1: 1.0681 (1.3792)  loss_giou_aux_2: 1.0557 (1.3621)  loss_giou_aux_3: 1.0474 (1.3526)  loss_giou_aux_4: 1.0366 (1.3473)  loss_giou_aux_5: 1.2001 (1.4690)  loss_giou_dn_0: 1.2093 (1.2929)  loss_giou_dn_1: 1.1821 (1.2769)  loss_giou_dn_2: 1.1471 (1.2709)  loss_giou_dn_3: 1.1393 (1.2706)  loss_giou_dn_4: 1.1351 (1.2726)  loss_giou_dn_5: 1.1378 (1.2756)  loss_vfl: 1.0707 (0.7724)  loss_vfl_aux_0: 0.9372 (0.6975)  loss_vfl_aux_1: 0.9509 (0.7173)  loss_vfl_aux_2: 0.9822 (0.7412)  loss_vfl_aux_3: 0.9978 (0.7542)  loss_vfl_aux_4: 1.0348 (0.7670)  loss_vfl_aux_5: 0.8441 (0.6378)  loss_vfl_dn_0: 0.4857 (0.5950)  loss_vfl_dn_1: 0.5110 (0.5882)  loss_vfl_dn_2: 0.5177 (0.5990)  loss_vfl_dn_3: 0.5253 (0.5940)  loss_vfl_dn_4: 0.5328 (0.5944)  loss_vfl_dn_5: 0.5260 (0.5923)  time: 0.3354  data: 0.0065  max mem: 12450
Epoch: [0]  [1200/7393]  eta: 0:33:32  lr: 0.000006  loss: 32.2655 (36.8816)  loss_bbox: 0.4248 (0.8591)  loss_bbox_aux_0: 0.5143 (0.9234)  loss_bbox_aux_1: 0.4667 (0.8962)  loss_bbox_aux_2: 0.4463 (0.8791)  loss_bbox_aux_3: 0.4455 (0.8695)  loss_bbox_aux_4: 0.4279 (0.8631)  loss_bbox_aux_5: 0.6375 (0.9904)  loss_bbox_dn_0: 0.7134 (0.8038)  loss_bbox_dn_1: 0.6682 (0.7884)  loss_bbox_dn_2: 0.6662 (0.7818)  loss_bbox_dn_3: 0.6698 (0.7805)  loss_bbox_dn_4: 0.6679 (0.7804)  loss_bbox_dn_5: 0.6672 (0.7814)  loss_giou: 0.9226 (1.3145)  loss_giou_aux_0: 0.9958 (1.3771)  loss_giou_aux_1: 0.9557 (1.3504)  loss_giou_aux_2: 0.9291 (1.3330)  loss_giou_aux_3: 0.9161 (1.3237)  loss_giou_aux_4: 0.9134 (1.3184)  loss_giou_aux_5: 1.1006 (1.4449)  loss_giou_dn_0: 1.1452 (1.2838)  loss_giou_dn_1: 1.0850 (1.2651)  loss_giou_dn_2: 1.0700 (1.2579)  loss_giou_dn_3: 1.0559 (1.2570)  loss_giou_dn_4: 1.0530 (1.2587)  loss_giou_dn_5: 1.0531 (1.2617)  loss_vfl: 1.2577 (0.8046)  loss_vfl_aux_0: 1.0739 (0.7259)  loss_vfl_aux_1: 1.1238 (0.7467)  loss_vfl_aux_2: 1.1499 (0.7711)  loss_vfl_aux_3: 1.1952 (0.7847)  loss_vfl_aux_4: 1.1808 (0.7979)  loss_vfl_aux_5: 1.0036 (0.6645)  loss_vfl_dn_0: 0.5123 (0.5879)  loss_vfl_dn_1: 0.5410 (0.5842)  loss_vfl_dn_2: 0.5590 (0.5957)  loss_vfl_dn_3: 0.5739 (0.5918)  loss_vfl_dn_4: 0.5866 (0.5924)  loss_vfl_dn_5: 0.5739 (0.5907)  time: 0.3302  data: 0.0065  max mem: 12450
Epoch: [0]  [1300/7393]  eta: 0:32:59  lr: 0.000007  loss: 31.3173 (36.4942)  loss_bbox: 0.4456 (0.8274)  loss_bbox_aux_0: 0.4956 (0.8915)  loss_bbox_aux_1: 0.4584 (0.8638)  loss_bbox_aux_2: 0.4574 (0.8470)  loss_bbox_aux_3: 0.4497 (0.8375)  loss_bbox_aux_4: 0.4452 (0.8313)  loss_bbox_aux_5: 0.5936 (0.9612)  loss_bbox_dn_0: 0.6215 (0.7952)  loss_bbox_dn_1: 0.5762 (0.7776)  loss_bbox_dn_2: 0.5609 (0.7706)  loss_bbox_dn_3: 0.5526 (0.7689)  loss_bbox_dn_4: 0.5517 (0.7688)  loss_bbox_dn_5: 0.5519 (0.7696)  loss_giou: 1.0211 (1.2885)  loss_giou_aux_0: 1.1033 (1.3522)  loss_giou_aux_1: 1.0722 (1.3245)  loss_giou_aux_2: 1.0489 (1.3067)  loss_giou_aux_3: 1.0470 (1.2975)  loss_giou_aux_4: 1.0364 (1.2923)  loss_giou_aux_5: 1.1958 (1.4235)  loss_giou_dn_0: 1.1923 (1.2754)  loss_giou_dn_1: 1.1411 (1.2541)  loss_giou_dn_2: 1.1237 (1.2456)  loss_giou_dn_3: 1.1176 (1.2438)  loss_giou_dn_4: 1.1198 (1.2453)  loss_giou_dn_5: 1.1216 (1.2481)  loss_vfl: 0.9967 (0.8312)  loss_vfl_aux_0: 0.8947 (0.7488)  loss_vfl_aux_1: 0.9274 (0.7708)  loss_vfl_aux_2: 0.9386 (0.7955)  loss_vfl_aux_3: 0.9544 (0.8094)  loss_vfl_aux_4: 0.9757 (0.8234)  loss_vfl_aux_5: 0.8677 (0.6875)  loss_vfl_dn_0: 0.4652 (0.5806)  loss_vfl_dn_1: 0.4853 (0.5793)  loss_vfl_dn_2: 0.5131 (0.5917)  loss_vfl_dn_3: 0.5295 (0.5890)  loss_vfl_dn_4: 0.5254 (0.5900)  loss_vfl_dn_5: 0.5276 (0.5888)  time: 0.3272  data: 0.0065  max mem: 12450
Epoch: [0]  [1400/7393]  eta: 0:32:28  lr: 0.000007  loss: 31.2165 (36.1413)  loss_bbox: 0.4119 (0.7998)  loss_bbox_aux_0: 0.4897 (0.8635)  loss_bbox_aux_1: 0.4367 (0.8355)  loss_bbox_aux_2: 0.4266 (0.8190)  loss_bbox_aux_3: 0.4297 (0.8098)  loss_bbox_aux_4: 0.4198 (0.8036)  loss_bbox_aux_5: 0.5973 (0.9351)  loss_bbox_dn_0: 0.6484 (0.7873)  loss_bbox_dn_1: 0.6136 (0.7676)  loss_bbox_dn_2: 0.6008 (0.7601)  loss_bbox_dn_3: 0.6037 (0.7581)  loss_bbox_dn_4: 0.6024 (0.7580)  loss_bbox_dn_5: 0.6023 (0.7588)  loss_giou: 0.8996 (1.2640)  loss_giou_aux_0: 0.9825 (1.3289)  loss_giou_aux_1: 0.9404 (1.3002)  loss_giou_aux_2: 0.9109 (1.2822)  loss_giou_aux_3: 0.9090 (1.2730)  loss_giou_aux_4: 0.9073 (1.2677)  loss_giou_aux_5: 1.1076 (1.4035)  loss_giou_dn_0: 1.1400 (1.2669)  loss_giou_dn_1: 1.0677 (1.2427)  loss_giou_dn_2: 1.0374 (1.2330)  loss_giou_dn_3: 1.0293 (1.2307)  loss_giou_dn_4: 1.0252 (1.2320)  loss_giou_dn_5: 1.0260 (1.2347)  loss_vfl: 1.1774 (0.8560)  loss_vfl_aux_0: 1.0113 (0.7695)  loss_vfl_aux_1: 1.0849 (0.7931)  loss_vfl_aux_2: 1.0954 (0.8177)  loss_vfl_aux_3: 1.1157 (0.8321)  loss_vfl_aux_4: 1.1348 (0.8468)  loss_vfl_aux_5: 0.9827 (0.7088)  loss_vfl_dn_0: 0.4801 (0.5745)  loss_vfl_dn_1: 0.5082 (0.5757)  loss_vfl_dn_2: 0.5374 (0.5888)  loss_vfl_dn_3: 0.5443 (0.5870)  loss_vfl_dn_4: 0.5472 (0.5884)  loss_vfl_dn_5: 0.5570 (0.5876)  time: 0.3736  data: 0.0062  max mem: 12450
Epoch: [0]  [1500/7393]  eta: 0:31:57  lr: 0.000008  loss: 30.5166 (35.7909)  loss_bbox: 0.3751 (0.7725)  loss_bbox_aux_0: 0.4268 (0.8365)  loss_bbox_aux_1: 0.3981 (0.8077)  loss_bbox_aux_2: 0.3829 (0.7915)  loss_bbox_aux_3: 0.3772 (0.7824)  loss_bbox_aux_4: 0.3736 (0.7763)  loss_bbox_aux_5: 0.5789 (0.9109)  loss_bbox_dn_0: 0.6596 (0.7807)  loss_bbox_dn_1: 0.6149 (0.7587)  loss_bbox_dn_2: 0.6024 (0.7507)  loss_bbox_dn_3: 0.5968 (0.7485)  loss_bbox_dn_4: 0.5940 (0.7483)  loss_bbox_dn_5: 0.5937 (0.7490)  loss_giou: 0.9683 (1.2389)  loss_giou_aux_0: 1.0269 (1.3054)  loss_giou_aux_1: 0.9955 (1.2752)  loss_giou_aux_2: 0.9855 (1.2572)  loss_giou_aux_3: 0.9766 (1.2480)  loss_giou_aux_4: 0.9682 (1.2426)  loss_giou_aux_5: 1.1381 (1.3834)  loss_giou_dn_0: 1.1480 (1.2580)  loss_giou_dn_1: 1.0890 (1.2306)  loss_giou_dn_2: 1.0531 (1.2196)  loss_giou_dn_3: 1.0426 (1.2167)  loss_giou_dn_4: 1.0351 (1.2177)  loss_giou_dn_5: 1.0343 (1.2203)  loss_vfl: 1.1130 (0.8798)  loss_vfl_aux_0: 0.9954 (0.7890)  loss_vfl_aux_1: 1.0370 (0.8147)  loss_vfl_aux_2: 1.0544 (0.8397)  loss_vfl_aux_3: 1.0835 (0.8541)  loss_vfl_aux_4: 1.0912 (0.8697)  loss_vfl_aux_5: 0.9826 (0.7294)  loss_vfl_dn_0: 0.4895 (0.5691)  loss_vfl_dn_1: 0.5202 (0.5724)  loss_vfl_dn_2: 0.5354 (0.5862)  loss_vfl_dn_3: 0.5530 (0.5854)  loss_vfl_dn_4: 0.5584 (0.5871)  loss_vfl_dn_5: 0.5507 (0.5867)  time: 0.3376  data: 0.0066  max mem: 12453
Epoch: [0]  [1600/7393]  eta: 0:31:24  lr: 0.000008  loss: 30.3913 (35.4819)  loss_bbox: 0.3804 (0.7498)  loss_bbox_aux_0: 0.4620 (0.8141)  loss_bbox_aux_1: 0.4254 (0.7848)  loss_bbox_aux_2: 0.3999 (0.7686)  loss_bbox_aux_3: 0.3975 (0.7597)  loss_bbox_aux_4: 0.3878 (0.7536)  loss_bbox_aux_5: 0.5596 (0.8906)  loss_bbox_dn_0: 0.6472 (0.7747)  loss_bbox_dn_1: 0.5812 (0.7509)  loss_bbox_dn_2: 0.5634 (0.7425)  loss_bbox_dn_3: 0.5590 (0.7401)  loss_bbox_dn_4: 0.5572 (0.7398)  loss_bbox_dn_5: 0.5574 (0.7405)  loss_giou: 0.7944 (1.2177)  loss_giou_aux_0: 0.9085 (1.2856)  loss_giou_aux_1: 0.8387 (1.2542)  loss_giou_aux_2: 0.8333 (1.2361)  loss_giou_aux_3: 0.8136 (1.2269)  loss_giou_aux_4: 0.8009 (1.2215)  loss_giou_aux_5: 1.0362 (1.3664)  loss_giou_dn_0: 1.0998 (1.2502)  loss_giou_dn_1: 1.0211 (1.2198)  loss_giou_dn_2: 0.9866 (1.2077)  loss_giou_dn_3: 0.9730 (1.2040)  loss_giou_dn_4: 0.9689 (1.2048)  loss_giou_dn_5: 0.9702 (1.2074)  loss_vfl: 1.1683 (0.8988)  loss_vfl_aux_0: 1.0182 (0.8045)  loss_vfl_aux_1: 1.1048 (0.8317)  loss_vfl_aux_2: 1.1350 (0.8572)  loss_vfl_aux_3: 1.1255 (0.8718)  loss_vfl_aux_4: 1.1491 (0.8879)  loss_vfl_aux_5: 1.0375 (0.7471)  loss_vfl_dn_0: 0.4963 (0.5638)  loss_vfl_dn_1: 0.5221 (0.5689)  loss_vfl_dn_2: 0.5529 (0.5834)  loss_vfl_dn_3: 0.5637 (0.5836)  loss_vfl_dn_4: 0.5622 (0.5857)  loss_vfl_dn_5: 0.5705 (0.5856)  time: 0.3191  data: 0.0065  max mem: 12453
Epoch: [0]  [1700/7393]  eta: 0:30:53  lr: 0.000009  loss: 29.2391 (35.1504)  loss_bbox: 0.3452 (0.7281)  loss_bbox_aux_0: 0.4158 (0.7920)  loss_bbox_aux_1: 0.3878 (0.7624)  loss_bbox_aux_2: 0.3640 (0.7467)  loss_bbox_aux_3: 0.3495 (0.7378)  loss_bbox_aux_4: 0.3487 (0.7318)  loss_bbox_aux_5: 0.4924 (0.8692)  loss_bbox_dn_0: 0.6410 (0.7654)  loss_bbox_dn_1: 0.5846 (0.7400)  loss_bbox_dn_2: 0.5654 (0.7313)  loss_bbox_dn_3: 0.5560 (0.7287)  loss_bbox_dn_4: 0.5541 (0.7284)  loss_bbox_dn_5: 0.5542 (0.7291)  loss_giou: 0.8472 (1.1992)  loss_giou_aux_0: 0.9295 (1.2680)  loss_giou_aux_1: 0.8792 (1.2357)  loss_giou_aux_2: 0.8601 (1.2175)  loss_giou_aux_3: 0.8538 (1.2085)  loss_giou_aux_4: 0.8453 (1.2030)  loss_giou_aux_5: 1.0577 (1.3510)  loss_giou_dn_0: 1.0966 (1.2426)  loss_giou_dn_1: 1.0201 (1.2096)  loss_giou_dn_2: 0.9876 (1.1965)  loss_giou_dn_3: 0.9840 (1.1924)  loss_giou_dn_4: 0.9811 (1.1930)  loss_giou_dn_5: 0.9805 (1.1954)  loss_vfl: 1.1753 (0.9128)  loss_vfl_aux_0: 1.0573 (0.8165)  loss_vfl_aux_1: 1.0828 (0.8449)  loss_vfl_aux_2: 1.1399 (0.8702)  loss_vfl_aux_3: 1.1387 (0.8850)  loss_vfl_aux_4: 1.1665 (0.9013)  loss_vfl_aux_5: 1.0392 (0.7621)  loss_vfl_dn_0: 0.4883 (0.5589)  loss_vfl_dn_1: 0.5131 (0.5653)  loss_vfl_dn_2: 0.5384 (0.5805)  loss_vfl_dn_3: 0.5451 (0.5815)  loss_vfl_dn_4: 0.5520 (0.5840)  loss_vfl_dn_5: 0.5594 (0.5841)  time: 0.3235  data: 0.0063  max mem: 12453
Epoch: [0]  [1800/7393]  eta: 0:30:24  lr: 0.000009  loss: 29.0291 (34.8631)  loss_bbox: 0.3263 (0.7092)  loss_bbox_aux_0: 0.3978 (0.7734)  loss_bbox_aux_1: 0.3526 (0.7432)  loss_bbox_aux_2: 0.3537 (0.7276)  loss_bbox_aux_3: 0.3360 (0.7189)  loss_bbox_aux_4: 0.3274 (0.7129)  loss_bbox_aux_5: 0.5070 (0.8516)  loss_bbox_dn_0: 0.5698 (0.7586)  loss_bbox_dn_1: 0.5322 (0.7314)  loss_bbox_dn_2: 0.5267 (0.7223)  loss_bbox_dn_3: 0.5192 (0.7195)  loss_bbox_dn_4: 0.5250 (0.7192)  loss_bbox_dn_5: 0.5249 (0.7198)  loss_giou: 0.8663 (1.1812)  loss_giou_aux_0: 0.9181 (1.2511)  loss_giou_aux_1: 0.8846 (1.2178)  loss_giou_aux_2: 0.8720 (1.1996)  loss_giou_aux_3: 0.8680 (1.1905)  loss_giou_aux_4: 0.8689 (1.1851)  loss_giou_aux_5: 1.0628 (1.3366)  loss_giou_dn_0: 1.0946 (1.2352)  loss_giou_dn_1: 1.0033 (1.1993)  loss_giou_dn_2: 0.9701 (1.1852)  loss_giou_dn_3: 0.9615 (1.1805)  loss_giou_dn_4: 0.9586 (1.1809)  loss_giou_dn_5: 0.9575 (1.1833)  loss_vfl: 1.1535 (0.9273)  loss_vfl_aux_0: 0.9740 (0.8283)  loss_vfl_aux_1: 1.0510 (0.8580)  loss_vfl_aux_2: 1.0517 (0.8836)  loss_vfl_aux_3: 1.0784 (0.8986)  loss_vfl_aux_4: 1.1267 (0.9152)  loss_vfl_aux_5: 0.9903 (0.7766)  loss_vfl_dn_0: 0.4816 (0.5547)  loss_vfl_dn_1: 0.5072 (0.5624)  loss_vfl_dn_2: 0.5242 (0.5782)  loss_vfl_dn_3: 0.5488 (0.5801)  loss_vfl_dn_4: 0.5499 (0.5830)  loss_vfl_dn_5: 0.5626 (0.5834)  time: 0.3135  data: 0.0063  max mem: 12453
Epoch: [0]  [1900/7393]  eta: 0:29:48  lr: 0.000010  loss: 28.5073 (34.5659)  loss_bbox: 0.3264 (0.6904)  loss_bbox_aux_0: 0.3679 (0.7543)  loss_bbox_aux_1: 0.3354 (0.7239)  loss_bbox_aux_2: 0.3410 (0.7086)  loss_bbox_aux_3: 0.3311 (0.7000)  loss_bbox_aux_4: 0.3213 (0.6941)  loss_bbox_aux_5: 0.4455 (0.8336)  loss_bbox_dn_0: 0.5630 (0.7512)  loss_bbox_dn_1: 0.5323 (0.7225)  loss_bbox_dn_2: 0.5102 (0.7131)  loss_bbox_dn_3: 0.5015 (0.7102)  loss_bbox_dn_4: 0.4994 (0.7098)  loss_bbox_dn_5: 0.4994 (0.7104)  loss_giou: 0.8570 (1.1641)  loss_giou_aux_0: 0.9231 (1.2345)  loss_giou_aux_1: 0.8887 (1.2005)  loss_giou_aux_2: 0.8668 (1.1825)  loss_giou_aux_3: 0.8636 (1.1735)  loss_giou_aux_4: 0.8594 (1.1681)  loss_giou_aux_5: 1.0308 (1.3219)  loss_giou_dn_0: 1.1011 (1.2281)  loss_giou_dn_1: 0.9959 (1.1895)  loss_giou_dn_2: 0.9623 (1.1745)  loss_giou_dn_3: 0.9492 (1.1694)  loss_giou_dn_4: 0.9485 (1.1696)  loss_giou_dn_5: 0.9485 (1.1718)  loss_vfl: 1.0919 (0.9389)  loss_vfl_aux_0: 0.9670 (0.8390)  loss_vfl_aux_1: 1.0163 (0.8696)  loss_vfl_aux_2: 1.0594 (0.8942)  loss_vfl_aux_3: 1.0399 (0.9094)  loss_vfl_aux_4: 1.0779 (0.9263)  loss_vfl_aux_5: 0.9748 (0.7898)  loss_vfl_dn_0: 0.4703 (0.5508)  loss_vfl_dn_1: 0.4922 (0.5594)  loss_vfl_dn_2: 0.5205 (0.5758)  loss_vfl_dn_3: 0.5332 (0.5783)  loss_vfl_dn_4: 0.5380 (0.5817)  loss_vfl_dn_5: 0.5409 (0.5824)  time: 0.3016  data: 0.0064  max mem: 12453
Epoch: [0]  [2000/7393]  eta: 0:29:13  lr: 0.000010  loss: 29.0039 (34.2999)  loss_bbox: 0.3717 (0.6738)  loss_bbox_aux_0: 0.4171 (0.7374)  loss_bbox_aux_1: 0.3835 (0.7069)  loss_bbox_aux_2: 0.3789 (0.6917)  loss_bbox_aux_3: 0.3736 (0.6832)  loss_bbox_aux_4: 0.3695 (0.6775)  loss_bbox_aux_5: 0.5042 (0.8178)  loss_bbox_dn_0: 0.6139 (0.7453)  loss_bbox_dn_1: 0.5508 (0.7150)  loss_bbox_dn_2: 0.5416 (0.7051)  loss_bbox_dn_3: 0.5437 (0.7021)  loss_bbox_dn_4: 0.5418 (0.7016)  loss_bbox_dn_5: 0.5422 (0.7022)  loss_giou: 0.8006 (1.1483)  loss_giou_aux_0: 0.8916 (1.2190)  loss_giou_aux_1: 0.8485 (1.1846)  loss_giou_aux_2: 0.8488 (1.1665)  loss_giou_aux_3: 0.8160 (1.1577)  loss_giou_aux_4: 0.8105 (1.1523)  loss_giou_aux_5: 1.0044 (1.3083)  loss_giou_dn_0: 1.1232 (1.2219)  loss_giou_dn_1: 1.0320 (1.1809)  loss_giou_dn_2: 0.9930 (1.1649)  loss_giou_dn_3: 0.9833 (1.1593)  loss_giou_dn_4: 0.9757 (1.1593)  loss_giou_dn_5: 0.9749 (1.1615)  loss_vfl: 1.1963 (0.9493)  loss_vfl_aux_0: 1.0470 (0.8493)  loss_vfl_aux_1: 1.0683 (0.8799)  loss_vfl_aux_2: 1.0921 (0.9045)  loss_vfl_aux_3: 1.1432 (0.9195)  loss_vfl_aux_4: 1.1596 (0.9363)  loss_vfl_aux_5: 0.9983 (0.8022)  loss_vfl_dn_0: 0.4645 (0.5472)  loss_vfl_dn_1: 0.4845 (0.5566)  loss_vfl_dn_2: 0.5104 (0.5734)  loss_vfl_dn_3: 0.5317 (0.5766)  loss_vfl_dn_4: 0.5455 (0.5803)  loss_vfl_dn_5: 0.5475 (0.5812)  time: 0.3201  data: 0.0067  max mem: 12453
Epoch: [0]  [2100/7393]  eta: 0:28:40  lr: 0.000010  loss: 28.4314 (34.0459)  loss_bbox: 0.3403 (0.6589)  loss_bbox_aux_0: 0.3853 (0.7222)  loss_bbox_aux_1: 0.3629 (0.6914)  loss_bbox_aux_2: 0.3444 (0.6765)  loss_bbox_aux_3: 0.3457 (0.6682)  loss_bbox_aux_4: 0.3384 (0.6625)  loss_bbox_aux_5: 0.4479 (0.8033)  loss_bbox_dn_0: 0.5842 (0.7397)  loss_bbox_dn_1: 0.5270 (0.7078)  loss_bbox_dn_2: 0.5057 (0.6975)  loss_bbox_dn_3: 0.4987 (0.6943)  loss_bbox_dn_4: 0.5011 (0.6938)  loss_bbox_dn_5: 0.5021 (0.6944)  loss_giou: 0.8253 (1.1339)  loss_giou_aux_0: 0.9026 (1.2048)  loss_giou_aux_1: 0.8649 (1.1700)  loss_giou_aux_2: 0.8458 (1.1520)  loss_giou_aux_3: 0.8399 (1.1433)  loss_giou_aux_4: 0.8303 (1.1379)  loss_giou_aux_5: 1.0088 (1.2959)  loss_giou_dn_0: 1.0862 (1.2157)  loss_giou_dn_1: 0.9876 (1.1723)  loss_giou_dn_2: 0.9482 (1.1553)  loss_giou_dn_3: 0.9326 (1.1492)  loss_giou_dn_4: 0.9272 (1.1491)  loss_giou_dn_5: 0.9259 (1.1512)  loss_vfl: 1.1301 (0.9572)  loss_vfl_aux_0: 1.0345 (0.8578)  loss_vfl_aux_1: 1.0426 (0.8886)  loss_vfl_aux_2: 1.0637 (0.9127)  loss_vfl_aux_3: 1.0941 (0.9273)  loss_vfl_aux_4: 1.1216 (0.9439)  loss_vfl_aux_5: 1.0678 (0.8133)  loss_vfl_dn_0: 0.4775 (0.5441)  loss_vfl_dn_1: 0.4968 (0.5540)  loss_vfl_dn_2: 0.5166 (0.5713)  loss_vfl_dn_3: 0.5396 (0.5752)  loss_vfl_dn_4: 0.5514 (0.5792)  loss_vfl_dn_5: 0.5665 (0.5803)  time: 0.3275  data: 0.0063  max mem: 12453
Epoch: [0]  [2200/7393]  eta: 0:28:08  lr: 0.000010  loss: 28.0436 (33.8208)  loss_bbox: 0.3280 (0.6455)  loss_bbox_aux_0: 0.3713 (0.7086)  loss_bbox_aux_1: 0.3580 (0.6776)  loss_bbox_aux_2: 0.3369 (0.6628)  loss_bbox_aux_3: 0.3334 (0.6547)  loss_bbox_aux_4: 0.3269 (0.6491)  loss_bbox_aux_5: 0.4587 (0.7909)  loss_bbox_dn_0: 0.5610 (0.7353)  loss_bbox_dn_1: 0.5080 (0.7021)  loss_bbox_dn_2: 0.4867 (0.6913)  loss_bbox_dn_3: 0.4813 (0.6879)  loss_bbox_dn_4: 0.4806 (0.6874)  loss_bbox_dn_5: 0.4806 (0.6879)  loss_giou: 0.7617 (1.1202)  loss_giou_aux_0: 0.8456 (1.1913)  loss_giou_aux_1: 0.8189 (1.1562)  loss_giou_aux_2: 0.7993 (1.1383)  loss_giou_aux_3: 0.7843 (1.1295)  loss_giou_aux_4: 0.7674 (1.1242)  loss_giou_aux_5: 0.9999 (1.2842)  loss_giou_dn_0: 1.0522 (1.2093)  loss_giou_dn_1: 0.9458 (1.1636)  loss_giou_dn_2: 0.9141 (1.1458)  loss_giou_dn_3: 0.9009 (1.1393)  loss_giou_dn_4: 0.8969 (1.1390)  loss_giou_dn_5: 0.8965 (1.1410)  loss_vfl: 1.1073 (0.9654)  loss_vfl_aux_0: 1.0118 (0.8666)  loss_vfl_aux_1: 1.0488 (0.8975)  loss_vfl_aux_2: 1.0738 (0.9212)  loss_vfl_aux_3: 1.0727 (0.9354)  loss_vfl_aux_4: 1.0988 (0.9518)  loss_vfl_aux_5: 1.0100 (0.8245)  loss_vfl_dn_0: 0.4854 (0.5415)  loss_vfl_dn_1: 0.5104 (0.5519)  loss_vfl_dn_2: 0.5339 (0.5695)  loss_vfl_dn_3: 0.5474 (0.5740)  loss_vfl_dn_4: 0.5570 (0.5784)  loss_vfl_dn_5: 0.5560 (0.5796)  time: 0.3222  data: 0.0061  max mem: 12453
Epoch: [0]  [2300/7393]  eta: 0:27:35  lr: 0.000010  loss: 27.7089 (33.5838)  loss_bbox: 0.2936 (0.6322)  loss_bbox_aux_0: 0.3509 (0.6950)  loss_bbox_aux_1: 0.3083 (0.6639)  loss_bbox_aux_2: 0.3003 (0.6492)  loss_bbox_aux_3: 0.3036 (0.6412)  loss_bbox_aux_4: 0.3053 (0.6358)  loss_bbox_aux_5: 0.4478 (0.7779)  loss_bbox_dn_0: 0.5630 (0.7297)  loss_bbox_dn_1: 0.4894 (0.6950)  loss_bbox_dn_2: 0.4827 (0.6838)  loss_bbox_dn_3: 0.4794 (0.6802)  loss_bbox_dn_4: 0.4791 (0.6796)  loss_bbox_dn_5: 0.4793 (0.6802)  loss_giou: 0.7943 (1.1072)  loss_giou_aux_0: 0.8761 (1.1786)  loss_giou_aux_1: 0.8207 (1.1431)  loss_giou_aux_2: 0.8087 (1.1253)  loss_giou_aux_3: 0.7995 (1.1165)  loss_giou_aux_4: 0.7955 (1.1112)  loss_giou_aux_5: 0.9954 (1.2728)  loss_giou_dn_0: 1.0683 (1.2034)  loss_giou_dn_1: 0.9803 (1.1553)  loss_giou_dn_2: 0.9344 (1.1365)  loss_giou_dn_3: 0.9150 (1.1296)  loss_giou_dn_4: 0.9166 (1.1291)  loss_giou_dn_5: 0.9181 (1.1311)  loss_vfl: 1.0723 (0.9718)  loss_vfl_aux_0: 1.0176 (0.8743)  loss_vfl_aux_1: 1.0314 (0.9053)  loss_vfl_aux_2: 1.0072 (0.9285)  loss_vfl_aux_3: 1.0567 (0.9422)  loss_vfl_aux_4: 1.0562 (0.9581)  loss_vfl_aux_5: 1.0565 (0.8344)  loss_vfl_dn_0: 0.4796 (0.5390)  loss_vfl_dn_1: 0.5034 (0.5499)  loss_vfl_dn_2: 0.5221 (0.5676)  loss_vfl_dn_3: 0.5455 (0.5728)  loss_vfl_dn_4: 0.5453 (0.5774)  loss_vfl_dn_5: 0.5530 (0.5789)  time: 0.3234  data: 0.0065  max mem: 12453
Epoch: [0]  [2400/7393]  eta: 0:27:01  lr: 0.000010  loss: 27.9683 (33.3528)  loss_bbox: 0.3176 (0.6195)  loss_bbox_aux_0: 0.3740 (0.6819)  loss_bbox_aux_1: 0.3399 (0.6508)  loss_bbox_aux_2: 0.3252 (0.6362)  loss_bbox_aux_3: 0.3261 (0.6284)  loss_bbox_aux_4: 0.3122 (0.6231)  loss_bbox_aux_5: 0.4631 (0.7653)  loss_bbox_dn_0: 0.5541 (0.7241)  loss_bbox_dn_1: 0.4970 (0.6882)  loss_bbox_dn_2: 0.4762 (0.6765)  loss_bbox_dn_3: 0.4723 (0.6728)  loss_bbox_dn_4: 0.4717 (0.6723)  loss_bbox_dn_5: 0.4720 (0.6727)  loss_giou: 0.8166 (1.0950)  loss_giou_aux_0: 0.9037 (1.1666)  loss_giou_aux_1: 0.8350 (1.1307)  loss_giou_aux_2: 0.8248 (1.1129)  loss_giou_aux_3: 0.8198 (1.1042)  loss_giou_aux_4: 0.8156 (1.0989)  loss_giou_aux_5: 1.0513 (1.2621)  loss_giou_dn_0: 1.0580 (1.1981)  loss_giou_dn_1: 0.9457 (1.1480)  loss_giou_dn_2: 0.9000 (1.1282)  loss_giou_dn_3: 0.8819 (1.1209)  loss_giou_dn_4: 0.8771 (1.1203)  loss_giou_dn_5: 0.8777 (1.1222)  loss_vfl: 1.0913 (0.9770)  loss_vfl_aux_0: 1.0029 (0.8808)  loss_vfl_aux_1: 1.0696 (0.9118)  loss_vfl_aux_2: 1.0522 (0.9343)  loss_vfl_aux_3: 1.0041 (0.9476)  loss_vfl_aux_4: 1.0488 (0.9632)  loss_vfl_aux_5: 1.0287 (0.8429)  loss_vfl_dn_0: 0.4729 (0.5365)  loss_vfl_dn_1: 0.4932 (0.5478)  loss_vfl_dn_2: 0.5125 (0.5656)  loss_vfl_dn_3: 0.5364 (0.5714)  loss_vfl_dn_4: 0.5511 (0.5763)  loss_vfl_dn_5: 0.5638 (0.5779)  time: 0.3214  data: 0.0065  max mem: 12453
Epoch: [0]  [2500/7393]  eta: 0:26:29  lr: 0.000010  loss: 28.2735 (33.1368)  loss_bbox: 0.3210 (0.6078)  loss_bbox_aux_0: 0.3480 (0.6699)  loss_bbox_aux_1: 0.3374 (0.6387)  loss_bbox_aux_2: 0.3286 (0.6243)  loss_bbox_aux_3: 0.3140 (0.6165)  loss_bbox_aux_4: 0.3112 (0.6114)  loss_bbox_aux_5: 0.4578 (0.7537)  loss_bbox_dn_0: 0.5964 (0.7191)  loss_bbox_dn_1: 0.5495 (0.6820)  loss_bbox_dn_2: 0.5226 (0.6699)  loss_bbox_dn_3: 0.5232 (0.6661)  loss_bbox_dn_4: 0.5229 (0.6655)  loss_bbox_dn_5: 0.5227 (0.6660)  loss_giou: 0.7886 (1.0835)  loss_giou_aux_0: 0.8678 (1.1550)  loss_giou_aux_1: 0.8451 (1.1190)  loss_giou_aux_2: 0.7953 (1.1013)  loss_giou_aux_3: 0.7945 (1.0927)  loss_giou_aux_4: 0.7902 (1.0874)  loss_giou_aux_5: 0.9580 (1.2517)  loss_giou_dn_0: 1.0668 (1.1929)  loss_giou_dn_1: 0.9694 (1.1407)  loss_giou_dn_2: 0.9250 (1.1201)  loss_giou_dn_3: 0.9031 (1.1124)  loss_giou_dn_4: 0.8970 (1.1117)  loss_giou_dn_5: 0.8972 (1.1135)  loss_vfl: 1.0960 (0.9814)  loss_vfl_aux_0: 1.0941 (0.8870)  loss_vfl_aux_1: 1.0913 (0.9177)  loss_vfl_aux_2: 1.0872 (0.9395)  loss_vfl_aux_3: 1.0881 (0.9525)  loss_vfl_aux_4: 1.1040 (0.9678)  loss_vfl_aux_5: 1.0795 (0.8512)  loss_vfl_dn_0: 0.4812 (0.5343)  loss_vfl_dn_1: 0.4970 (0.5460)  loss_vfl_dn_2: 0.5313 (0.5640)  loss_vfl_dn_3: 0.5497 (0.5701)  loss_vfl_dn_4: 0.5582 (0.5753)  loss_vfl_dn_5: 0.5648 (0.5771)  time: 0.3394  data: 0.0066  max mem: 12453
Epoch: [0]  [2600/7393]  eta: 0:25:55  lr: 0.000010  loss: 27.9192 (32.9383)  loss_bbox: 0.3358 (0.5974)  loss_bbox_aux_0: 0.3922 (0.6592)  loss_bbox_aux_1: 0.3621 (0.6279)  loss_bbox_aux_2: 0.3608 (0.6137)  loss_bbox_aux_3: 0.3382 (0.6060)  loss_bbox_aux_4: 0.3340 (0.6009)  loss_bbox_aux_5: 0.5006 (0.7436)  loss_bbox_dn_0: 0.5902 (0.7155)  loss_bbox_dn_1: 0.5278 (0.6770)  loss_bbox_dn_2: 0.5047 (0.6644)  loss_bbox_dn_3: 0.5064 (0.6605)  loss_bbox_dn_4: 0.5069 (0.6598)  loss_bbox_dn_5: 0.5068 (0.6602)  loss_giou: 0.7971 (1.0720)  loss_giou_aux_0: 0.8687 (1.1436)  loss_giou_aux_1: 0.8412 (1.1074)  loss_giou_aux_2: 0.8175 (1.0897)  loss_giou_aux_3: 0.8059 (1.0811)  loss_giou_aux_4: 0.7998 (1.0759)  loss_giou_aux_5: 0.9665 (1.2411)  loss_giou_dn_0: 1.0461 (1.1877)  loss_giou_dn_1: 0.9418 (1.1334)  loss_giou_dn_2: 0.8988 (1.1120)  loss_giou_dn_3: 0.8873 (1.1039)  loss_giou_dn_4: 0.8829 (1.1030)  loss_giou_dn_5: 0.8833 (1.1048)  loss_vfl: 1.0720 (0.9859)  loss_vfl_aux_0: 1.0158 (0.8937)  loss_vfl_aux_1: 1.0372 (0.9242)  loss_vfl_aux_2: 1.0477 (0.9450)  loss_vfl_aux_3: 1.0657 (0.9575)  loss_vfl_aux_4: 1.0762 (0.9723)  loss_vfl_aux_5: 1.0495 (0.8596)  loss_vfl_dn_0: 0.4818 (0.5324)  loss_vfl_dn_1: 0.5020 (0.5444)  loss_vfl_dn_2: 0.5104 (0.5623)  loss_vfl_dn_3: 0.5258 (0.5689)  loss_vfl_dn_4: 0.5353 (0.5743)  loss_vfl_dn_5: 0.5385 (0.5762)  time: 0.3078  data: 0.0063  max mem: 12453
Epoch: [0]  [2700/7393]  eta: 0:25:23  lr: 0.000010  loss: 27.1457 (32.7498)  loss_bbox: 0.3208 (0.5880)  loss_bbox_aux_0: 0.3694 (0.6494)  loss_bbox_aux_1: 0.3414 (0.6180)  loss_bbox_aux_2: 0.3247 (0.6039)  loss_bbox_aux_3: 0.3167 (0.5964)  loss_bbox_aux_4: 0.3158 (0.5914)  loss_bbox_aux_5: 0.4640 (0.7341)  loss_bbox_dn_0: 0.5936 (0.7108)  loss_bbox_dn_1: 0.5021 (0.6712)  loss_bbox_dn_2: 0.4815 (0.6583)  loss_bbox_dn_3: 0.4731 (0.6543)  loss_bbox_dn_4: 0.4728 (0.6535)  loss_bbox_dn_5: 0.4727 (0.6540)  loss_giou: 0.7709 (1.0627)  loss_giou_aux_0: 0.8688 (1.1342)  loss_giou_aux_1: 0.8159 (1.0977)  loss_giou_aux_2: 0.7903 (1.0802)  loss_giou_aux_3: 0.7841 (1.0717)  loss_giou_aux_4: 0.7790 (1.0665)  loss_giou_aux_5: 0.9933 (1.2324)  loss_giou_dn_0: 1.0435 (1.1829)  loss_giou_dn_1: 0.9486 (1.1269)  loss_giou_dn_2: 0.9031 (1.1047)  loss_giou_dn_3: 0.8840 (1.0964)  loss_giou_dn_4: 0.8776 (1.0954)  loss_giou_dn_5: 0.8781 (1.0970)  loss_vfl: 1.0509 (0.9889)  loss_vfl_aux_0: 1.0356 (0.8986)  loss_vfl_aux_1: 1.0677 (0.9289)  loss_vfl_aux_2: 1.0693 (0.9489)  loss_vfl_aux_3: 1.0374 (0.9609)  loss_vfl_aux_4: 1.0355 (0.9754)  loss_vfl_aux_5: 1.0640 (0.8660)  loss_vfl_dn_0: 0.4850 (0.5306)  loss_vfl_dn_1: 0.5043 (0.5428)  loss_vfl_dn_2: 0.5243 (0.5606)  loss_vfl_dn_3: 0.5302 (0.5676)  loss_vfl_dn_4: 0.5434 (0.5733)  loss_vfl_dn_5: 0.5493 (0.5753)  time: 0.3277  data: 0.0062  max mem: 12453
Epoch: [0]  [2800/7393]  eta: 0:24:52  lr: 0.000010  loss: 27.0550 (32.5689)  loss_bbox: 0.3078 (0.5787)  loss_bbox_aux_0: 0.3408 (0.6396)  loss_bbox_aux_1: 0.3324 (0.6083)  loss_bbox_aux_2: 0.3216 (0.5945)  loss_bbox_aux_3: 0.3100 (0.5870)  loss_bbox_aux_4: 0.3094 (0.5820)  loss_bbox_aux_5: 0.4351 (0.7247)  loss_bbox_dn_0: 0.5923 (0.7071)  loss_bbox_dn_1: 0.5248 (0.6664)  loss_bbox_dn_2: 0.4950 (0.6533)  loss_bbox_dn_3: 0.4875 (0.6491)  loss_bbox_dn_4: 0.4821 (0.6484)  loss_bbox_dn_5: 0.4824 (0.6488)  loss_giou: 0.7249 (1.0531)  loss_giou_aux_0: 0.7640 (1.1245)  loss_giou_aux_1: 0.7260 (1.0880)  loss_giou_aux_2: 0.7335 (1.0705)  loss_giou_aux_3: 0.7358 (1.0620)  loss_giou_aux_4: 0.7275 (1.0569)  loss_giou_aux_5: 0.9390 (1.2237)  loss_giou_dn_0: 1.0422 (1.1782)  loss_giou_dn_1: 0.9228 (1.1205)  loss_giou_dn_2: 0.8909 (1.0977)  loss_giou_dn_3: 0.8770 (1.0891)  loss_giou_dn_4: 0.8722 (1.0879)  loss_giou_dn_5: 0.8712 (1.0895)  loss_vfl: 1.0779 (0.9919)  loss_vfl_aux_0: 1.0494 (0.9039)  loss_vfl_aux_1: 1.0850 (0.9336)  loss_vfl_aux_2: 1.0483 (0.9528)  loss_vfl_aux_3: 1.0511 (0.9643)  loss_vfl_aux_4: 1.0367 (0.9785)  loss_vfl_aux_5: 1.0138 (0.8722)  loss_vfl_dn_0: 0.4876 (0.5289)  loss_vfl_dn_1: 0.5055 (0.5413)  loss_vfl_dn_2: 0.5151 (0.5590)  loss_vfl_dn_3: 0.5227 (0.5664)  loss_vfl_dn_4: 0.5390 (0.5723)  loss_vfl_dn_5: 0.5474 (0.5745)  time: 0.3266  data: 0.0070  max mem: 12454
Epoch: [0]  [2900/7393]  eta: 0:24:19  lr: 0.000010  loss: 26.1372 (32.3834)  loss_bbox: 0.2752 (0.5697)  loss_bbox_aux_0: 0.3061 (0.6301)  loss_bbox_aux_1: 0.2898 (0.5988)  loss_bbox_aux_2: 0.2748 (0.5852)  loss_bbox_aux_3: 0.2745 (0.5778)  loss_bbox_aux_4: 0.2748 (0.5730)  loss_bbox_aux_5: 0.3819 (0.7153)  loss_bbox_dn_0: 0.4737 (0.7025)  loss_bbox_dn_1: 0.4203 (0.6609)  loss_bbox_dn_2: 0.4044 (0.6473)  loss_bbox_dn_3: 0.3970 (0.6430)  loss_bbox_dn_4: 0.3961 (0.6423)  loss_bbox_dn_5: 0.3965 (0.6427)  loss_giou: 0.7655 (1.0442)  loss_giou_aux_0: 0.8099 (1.1152)  loss_giou_aux_1: 0.7787 (1.0786)  loss_giou_aux_2: 0.7685 (1.0614)  loss_giou_aux_3: 0.7575 (1.0530)  loss_giou_aux_4: 0.7631 (1.0480)  loss_giou_aux_5: 0.9703 (1.2151)  loss_giou_dn_0: 1.0374 (1.1735)  loss_giou_dn_1: 0.9315 (1.1141)  loss_giou_dn_2: 0.8904 (1.0907)  loss_giou_dn_3: 0.8752 (1.0818)  loss_giou_dn_4: 0.8683 (1.0805)  loss_giou_dn_5: 0.8686 (1.0821)  loss_vfl: 1.0344 (0.9940)  loss_vfl_aux_0: 1.0038 (0.9086)  loss_vfl_aux_1: 1.0140 (0.9378)  loss_vfl_aux_2: 0.9857 (0.9560)  loss_vfl_aux_3: 0.9757 (0.9670)  loss_vfl_aux_4: 1.0143 (0.9807)  loss_vfl_aux_5: 1.0484 (0.8779)  loss_vfl_dn_0: 0.4824 (0.5272)  loss_vfl_dn_1: 0.4997 (0.5399)  loss_vfl_dn_2: 0.5130 (0.5575)  loss_vfl_dn_3: 0.5260 (0.5651)  loss_vfl_dn_4: 0.5350 (0.5713)  loss_vfl_dn_5: 0.5504 (0.5736)  time: 0.3093  data: 0.0063  max mem: 12454
Epoch: [0]  [3000/7393]  eta: 0:23:46  lr: 0.000010  loss: 27.0482 (32.2162)  loss_bbox: 0.2990 (0.5615)  loss_bbox_aux_0: 0.3202 (0.6213)  loss_bbox_aux_1: 0.3052 (0.5901)  loss_bbox_aux_2: 0.3055 (0.5767)  loss_bbox_aux_3: 0.3040 (0.5695)  loss_bbox_aux_4: 0.3009 (0.5647)  loss_bbox_aux_5: 0.4181 (0.7068)  loss_bbox_dn_0: 0.5719 (0.6991)  loss_bbox_dn_1: 0.4775 (0.6564)  loss_bbox_dn_2: 0.4503 (0.6426)  loss_bbox_dn_3: 0.4414 (0.6382)  loss_bbox_dn_4: 0.4428 (0.6374)  loss_bbox_dn_5: 0.4435 (0.6378)  loss_giou: 0.7819 (1.0353)  loss_giou_aux_0: 0.8318 (1.1061)  loss_giou_aux_1: 0.8039 (1.0695)  loss_giou_aux_2: 0.7808 (1.0524)  loss_giou_aux_3: 0.7894 (1.0441)  loss_giou_aux_4: 0.7840 (1.0391)  loss_giou_aux_5: 0.9580 (1.2067)  loss_giou_dn_0: 1.0408 (1.1690)  loss_giou_dn_1: 0.9168 (1.1080)  loss_giou_dn_2: 0.8654 (1.0839)  loss_giou_dn_3: 0.8507 (1.0748)  loss_giou_dn_4: 0.8489 (1.0733)  loss_giou_dn_5: 0.8491 (1.0749)  loss_vfl: 1.0672 (0.9964)  loss_vfl_aux_0: 1.0513 (0.9137)  loss_vfl_aux_1: 1.0996 (0.9422)  loss_vfl_aux_2: 1.0485 (0.9596)  loss_vfl_aux_3: 1.0376 (0.9699)  loss_vfl_aux_4: 1.0538 (0.9831)  loss_vfl_aux_5: 1.0287 (0.8839)  loss_vfl_dn_0: 0.4859 (0.5259)  loss_vfl_dn_1: 0.5054 (0.5387)  loss_vfl_dn_2: 0.5212 (0.5562)  loss_vfl_dn_3: 0.5400 (0.5641)  loss_vfl_dn_4: 0.5481 (0.5705)  loss_vfl_dn_5: 0.5585 (0.5730)  time: 0.3002  data: 0.0062  max mem: 12454
Epoch: [0]  [3100/7393]  eta: 0:23:16  lr: 0.000010  loss: 26.8747 (32.0609)  loss_bbox: 0.2790 (0.5540)  loss_bbox_aux_0: 0.3252 (0.6133)  loss_bbox_aux_1: 0.3016 (0.5822)  loss_bbox_aux_2: 0.2881 (0.5689)  loss_bbox_aux_3: 0.2877 (0.5618)  loss_bbox_aux_4: 0.2786 (0.5571)  loss_bbox_aux_5: 0.4467 (0.6993)  loss_bbox_dn_0: 0.6164 (0.6964)  loss_bbox_dn_1: 0.5427 (0.6526)  loss_bbox_dn_2: 0.5159 (0.6384)  loss_bbox_dn_3: 0.5048 (0.6339)  loss_bbox_dn_4: 0.5017 (0.6330)  loss_bbox_dn_5: 0.5018 (0.6334)  loss_giou: 0.7448 (1.0263)  loss_giou_aux_0: 0.8289 (1.0969)  loss_giou_aux_1: 0.8019 (1.0602)  loss_giou_aux_2: 0.7620 (1.0432)  loss_giou_aux_3: 0.7639 (1.0350)  loss_giou_aux_4: 0.7520 (1.0301)  loss_giou_aux_5: 0.9561 (1.1983)  loss_giou_dn_0: 1.0138 (1.1644)  loss_giou_dn_1: 0.8982 (1.1017)  loss_giou_dn_2: 0.8648 (1.0769)  loss_giou_dn_3: 0.8468 (1.0674)  loss_giou_dn_4: 0.8412 (1.0659)  loss_giou_dn_5: 0.8414 (1.0674)  loss_vfl: 0.9841 (0.9995)  loss_vfl_aux_0: 1.0117 (0.9191)  loss_vfl_aux_1: 1.0141 (0.9471)  loss_vfl_aux_2: 1.0243 (0.9637)  loss_vfl_aux_3: 1.0048 (0.9734)  loss_vfl_aux_4: 0.9835 (0.9863)  loss_vfl_aux_5: 1.0034 (0.8904)  loss_vfl_dn_0: 0.4914 (0.5247)  loss_vfl_dn_1: 0.5073 (0.5377)  loss_vfl_dn_2: 0.5288 (0.5551)  loss_vfl_dn_3: 0.5501 (0.5633)  loss_vfl_dn_4: 0.5673 (0.5700)  loss_vfl_dn_5: 0.5696 (0.5726)  time: 0.3912  data: 0.0063  max mem: 12454
Epoch: [0]  [3200/7393]  eta: 0:22:43  lr: 0.000010  loss: 26.4143 (31.8964)  loss_bbox: 0.3019 (0.5462)  loss_bbox_aux_0: 0.3454 (0.6050)  loss_bbox_aux_1: 0.3294 (0.5740)  loss_bbox_aux_2: 0.3140 (0.5609)  loss_bbox_aux_3: 0.3102 (0.5540)  loss_bbox_aux_4: 0.3024 (0.5494)  loss_bbox_aux_5: 0.4253 (0.6912)  loss_bbox_dn_0: 0.5589 (0.6924)  loss_bbox_dn_1: 0.4780 (0.6477)  loss_bbox_dn_2: 0.4572 (0.6332)  loss_bbox_dn_3: 0.4489 (0.6286)  loss_bbox_dn_4: 0.4488 (0.6277)  loss_bbox_dn_5: 0.4489 (0.6281)  loss_giou: 0.7345 (1.0184)  loss_giou_aux_0: 0.8038 (1.0885)  loss_giou_aux_1: 0.7584 (1.0520)  loss_giou_aux_2: 0.7460 (1.0351)  loss_giou_aux_3: 0.7442 (1.0270)  loss_giou_aux_4: 0.7355 (1.0221)  loss_giou_aux_5: 0.9302 (1.1907)  loss_giou_dn_0: 1.0188 (1.1604)  loss_giou_dn_1: 0.8968 (1.0961)  loss_giou_dn_2: 0.8493 (1.0708)  loss_giou_dn_3: 0.8343 (1.0611)  loss_giou_dn_4: 0.8302 (1.0595)  loss_giou_dn_5: 0.8304 (1.0609)  loss_vfl: 1.0077 (1.0007)  loss_vfl_aux_0: 1.0782 (0.9232)  loss_vfl_aux_1: 1.0771 (0.9504)  loss_vfl_aux_2: 1.0685 (0.9662)  loss_vfl_aux_3: 1.0358 (0.9752)  loss_vfl_aux_4: 1.0249 (0.9876)  loss_vfl_aux_5: 1.0166 (0.8950)  loss_vfl_dn_0: 0.4831 (0.5235)  loss_vfl_dn_1: 0.4959 (0.5366)  loss_vfl_dn_2: 0.5060 (0.5538)  loss_vfl_dn_3: 0.5264 (0.5623)  loss_vfl_dn_4: 0.5397 (0.5691)  loss_vfl_dn_5: 0.5477 (0.5719)  time: 0.3121  data: 0.0060  max mem: 12454
Epoch: [0]  [3300/7393]  eta: 0:22:11  lr: 0.000010  loss: 26.4565 (31.7427)  loss_bbox: 0.2847 (0.5393)  loss_bbox_aux_0: 0.3297 (0.5976)  loss_bbox_aux_1: 0.3104 (0.5666)  loss_bbox_aux_2: 0.2968 (0.5537)  loss_bbox_aux_3: 0.2976 (0.5469)  loss_bbox_aux_4: 0.2859 (0.5424)  loss_bbox_aux_5: 0.4086 (0.6841)  loss_bbox_dn_0: 0.5290 (0.6894)  loss_bbox_dn_1: 0.4634 (0.6437)  loss_bbox_dn_2: 0.4436 (0.6289)  loss_bbox_dn_3: 0.4384 (0.6242)  loss_bbox_dn_4: 0.4375 (0.6233)  loss_bbox_dn_5: 0.4376 (0.6237)  loss_giou: 0.7440 (1.0105)  loss_giou_aux_0: 0.7809 (1.0804)  loss_giou_aux_1: 0.7643 (1.0438)  loss_giou_aux_2: 0.7252 (1.0270)  loss_giou_aux_3: 0.7344 (1.0190)  loss_giou_aux_4: 0.7537 (1.0142)  loss_giou_aux_5: 0.9211 (1.1832)  loss_giou_dn_0: 0.9988 (1.1563)  loss_giou_dn_1: 0.9056 (1.0905)  loss_giou_dn_2: 0.8636 (1.0647)  loss_giou_dn_3: 0.8369 (1.0548)  loss_giou_dn_4: 0.8329 (1.0531)  loss_giou_dn_5: 0.8333 (1.0545)  loss_vfl: 1.0388 (1.0019)  loss_vfl_aux_0: 1.0808 (0.9269)  loss_vfl_aux_1: 1.0557 (0.9537)  loss_vfl_aux_2: 1.0577 (0.9685)  loss_vfl_aux_3: 1.0331 (0.9770)  loss_vfl_aux_4: 1.0393 (0.9891)  loss_vfl_aux_5: 1.0475 (0.8995)  loss_vfl_dn_0: 0.4884 (0.5223)  loss_vfl_dn_1: 0.4937 (0.5355)  loss_vfl_dn_2: 0.5079 (0.5524)  loss_vfl_dn_3: 0.5074 (0.5610)  loss_vfl_dn_4: 0.5255 (0.5681)  loss_vfl_dn_5: 0.5375 (0.5710)  time: 0.3384  data: 0.0064  max mem: 12454
Epoch: [0]  [3400/7393]  eta: 0:21:37  lr: 0.000010  loss: 25.9483 (31.5900)  loss_bbox: 0.2720 (0.5325)  loss_bbox_aux_0: 0.3171 (0.5901)  loss_bbox_aux_1: 0.2878 (0.5593)  loss_bbox_aux_2: 0.2832 (0.5466)  loss_bbox_aux_3: 0.2749 (0.5400)  loss_bbox_aux_4: 0.2750 (0.5355)  loss_bbox_aux_5: 0.3977 (0.6768)  loss_bbox_dn_0: 0.5441 (0.6857)  loss_bbox_dn_1: 0.4677 (0.6392)  loss_bbox_dn_2: 0.4455 (0.6241)  loss_bbox_dn_3: 0.4399 (0.6193)  loss_bbox_dn_4: 0.4397 (0.6184)  loss_bbox_dn_5: 0.4397 (0.6188)  loss_giou: 0.7309 (1.0035)  loss_giou_aux_0: 0.7809 (1.0730)  loss_giou_aux_1: 0.7492 (1.0365)  loss_giou_aux_2: 0.7452 (1.0200)  loss_giou_aux_3: 0.7409 (1.0120)  loss_giou_aux_4: 0.7399 (1.0072)  loss_giou_aux_5: 0.9126 (1.1763)  loss_giou_dn_0: 0.9976 (1.1523)  loss_giou_dn_1: 0.8905 (1.0852)  loss_giou_dn_2: 0.8366 (1.0588)  loss_giou_dn_3: 0.8186 (1.0487)  loss_giou_dn_4: 0.8100 (1.0469)  loss_giou_dn_5: 0.8099 (1.0483)  loss_vfl: 1.0138 (1.0025)  loss_vfl_aux_0: 1.0444 (0.9302)  loss_vfl_aux_1: 1.0291 (0.9564)  loss_vfl_aux_2: 1.0475 (0.9702)  loss_vfl_aux_3: 1.0458 (0.9781)  loss_vfl_aux_4: 1.0114 (0.9899)  loss_vfl_aux_5: 1.0530 (0.9036)  loss_vfl_dn_0: 0.4859 (0.5212)  loss_vfl_dn_1: 0.5028 (0.5345)  loss_vfl_dn_2: 0.5101 (0.5511)  loss_vfl_dn_3: 0.5282 (0.5599)  loss_vfl_dn_4: 0.5414 (0.5671)  loss_vfl_dn_5: 0.5490 (0.5701)  time: 0.3244  data: 0.0063  max mem: 12454
Epoch: [0]  [3500/7393]  eta: 0:21:05  lr: 0.000010  loss: 26.7088 (31.4511)  loss_bbox: 0.3270 (0.5263)  loss_bbox_aux_0: 0.3767 (0.5834)  loss_bbox_aux_1: 0.3227 (0.5527)  loss_bbox_aux_2: 0.3346 (0.5402)  loss_bbox_aux_3: 0.3428 (0.5337)  loss_bbox_aux_4: 0.3226 (0.5293)  loss_bbox_aux_5: 0.4371 (0.6703)  loss_bbox_dn_0: 0.5843 (0.6830)  loss_bbox_dn_1: 0.5149 (0.6356)  loss_bbox_dn_2: 0.4821 (0.6203)  loss_bbox_dn_3: 0.4688 (0.6154)  loss_bbox_dn_4: 0.4738 (0.6145)  loss_bbox_dn_5: 0.4743 (0.6149)  loss_giou: 0.7845 (0.9964)  loss_giou_aux_0: 0.8518 (1.0657)  loss_giou_aux_1: 0.8106 (1.0292)  loss_giou_aux_2: 0.7962 (1.0127)  loss_giou_aux_3: 0.7890 (1.0048)  loss_giou_aux_4: 0.7805 (1.0001)  loss_giou_aux_5: 1.0220 (1.1697)  loss_giou_dn_0: 1.0225 (1.1484)  loss_giou_dn_1: 0.9192 (1.0799)  loss_giou_dn_2: 0.8692 (1.0530)  loss_giou_dn_3: 0.8601 (1.0428)  loss_giou_dn_4: 0.8558 (1.0409)  loss_giou_dn_5: 0.8558 (1.0422)  loss_vfl: 0.9329 (1.0036)  loss_vfl_aux_0: 0.9358 (0.9337)  loss_vfl_aux_1: 0.9840 (0.9593)  loss_vfl_aux_2: 0.9312 (0.9724)  loss_vfl_aux_3: 0.9506 (0.9796)  loss_vfl_aux_4: 0.9566 (0.9911)  loss_vfl_aux_5: 0.9804 (0.9076)  loss_vfl_dn_0: 0.4799 (0.5203)  loss_vfl_dn_1: 0.4850 (0.5335)  loss_vfl_dn_2: 0.4879 (0.5500)  loss_vfl_dn_3: 0.5008 (0.5589)  loss_vfl_dn_4: 0.5160 (0.5662)  loss_vfl_dn_5: 0.5162 (0.5694)  time: 0.3405  data: 0.0061  max mem: 12454
Epoch: [0]  [3600/7393]  eta: 0:20:32  lr: 0.000010  loss: 25.9743 (31.3105)  loss_bbox: 0.2890 (0.5201)  loss_bbox_aux_0: 0.3497 (0.5768)  loss_bbox_aux_1: 0.3015 (0.5461)  loss_bbox_aux_2: 0.2839 (0.5338)  loss_bbox_aux_3: 0.2796 (0.5273)  loss_bbox_aux_4: 0.2930 (0.5231)  loss_bbox_aux_5: 0.4244 (0.6639)  loss_bbox_dn_0: 0.5797 (0.6795)  loss_bbox_dn_1: 0.4937 (0.6314)  loss_bbox_dn_2: 0.4642 (0.6159)  loss_bbox_dn_3: 0.4628 (0.6110)  loss_bbox_dn_4: 0.4615 (0.6100)  loss_bbox_dn_5: 0.4616 (0.6104)  loss_giou: 0.6733 (0.9897)  loss_giou_aux_0: 0.7629 (1.0588)  loss_giou_aux_1: 0.7040 (1.0222)  loss_giou_aux_2: 0.6815 (1.0058)  loss_giou_aux_3: 0.6738 (0.9980)  loss_giou_aux_4: 0.6719 (0.9933)  loss_giou_aux_5: 0.8758 (1.1634)  loss_giou_dn_0: 0.9642 (1.1445)  loss_giou_dn_1: 0.8338 (1.0747)  loss_giou_dn_2: 0.7885 (1.0474)  loss_giou_dn_3: 0.7741 (1.0370)  loss_giou_dn_4: 0.7694 (1.0350)  loss_giou_dn_5: 0.7695 (1.0363)  loss_vfl: 1.0412 (1.0045)  loss_vfl_aux_0: 1.0600 (0.9368)  loss_vfl_aux_1: 1.0854 (0.9621)  loss_vfl_aux_2: 1.0673 (0.9743)  loss_vfl_aux_3: 1.0548 (0.9811)  loss_vfl_aux_4: 1.0380 (0.9921)  loss_vfl_aux_5: 1.0949 (0.9111)  loss_vfl_dn_0: 0.4968 (0.5194)  loss_vfl_dn_1: 0.5077 (0.5327)  loss_vfl_dn_2: 0.5264 (0.5488)  loss_vfl_dn_3: 0.5368 (0.5579)  loss_vfl_dn_4: 0.5483 (0.5655)  loss_vfl_dn_5: 0.5610 (0.5688)  time: 0.3173  data: 0.0063  max mem: 12454
Epoch: [0]  [3700/7393]  eta: 0:20:00  lr: 0.000010  loss: 26.1208 (31.1766)  loss_bbox: 0.3037 (0.5142)  loss_bbox_aux_0: 0.3448 (0.5704)  loss_bbox_aux_1: 0.3192 (0.5399)  loss_bbox_aux_2: 0.3181 (0.5276)  loss_bbox_aux_3: 0.3074 (0.5213)  loss_bbox_aux_4: 0.3084 (0.5172)  loss_bbox_aux_5: 0.4274 (0.6578)  loss_bbox_dn_0: 0.5314 (0.6762)  loss_bbox_dn_1: 0.4598 (0.6273)  loss_bbox_dn_2: 0.4358 (0.6116)  loss_bbox_dn_3: 0.4291 (0.6066)  loss_bbox_dn_4: 0.4262 (0.6057)  loss_bbox_dn_5: 0.4261 (0.6060)  loss_giou: 0.8117 (0.9839)  loss_giou_aux_0: 0.8555 (1.0528)  loss_giou_aux_1: 0.8216 (1.0162)  loss_giou_aux_2: 0.8046 (0.9999)  loss_giou_aux_3: 0.8028 (0.9921)  loss_giou_aux_4: 0.8107 (0.9875)  loss_giou_aux_5: 0.9741 (1.1579)  loss_giou_dn_0: 1.0220 (1.1409)  loss_giou_dn_1: 0.9098 (1.0699)  loss_giou_dn_2: 0.8582 (1.0422)  loss_giou_dn_3: 0.8456 (1.0316)  loss_giou_dn_4: 0.8415 (1.0296)  loss_giou_dn_5: 0.8418 (1.0309)  loss_vfl: 0.9749 (1.0046)  loss_vfl_aux_0: 0.9442 (0.9392)  loss_vfl_aux_1: 0.9207 (0.9642)  loss_vfl_aux_2: 0.9351 (0.9755)  loss_vfl_aux_3: 0.9075 (0.9818)  loss_vfl_aux_4: 0.9352 (0.9924)  loss_vfl_aux_5: 0.9363 (0.9140)  loss_vfl_dn_0: 0.4822 (0.5187)  loss_vfl_dn_1: 0.4984 (0.5318)  loss_vfl_dn_2: 0.5096 (0.5477)  loss_vfl_dn_3: 0.5180 (0.5569)  loss_vfl_dn_4: 0.5323 (0.5646)  loss_vfl_dn_5: 0.5400 (0.5680)  time: 0.3245  data: 0.0065  max mem: 12454
Epoch: [0]  [3800/7393]  eta: 0:19:28  lr: 0.000010  loss: 25.7954 (31.0501)  loss_bbox: 0.2856 (0.5087)  loss_bbox_aux_0: 0.3140 (0.5645)  loss_bbox_aux_1: 0.3036 (0.5340)  loss_bbox_aux_2: 0.2900 (0.5219)  loss_bbox_aux_3: 0.2959 (0.5157)  loss_bbox_aux_4: 0.2916 (0.5116)  loss_bbox_aux_5: 0.4015 (0.6520)  loss_bbox_dn_0: 0.5403 (0.6733)  loss_bbox_dn_1: 0.4705 (0.6237)  loss_bbox_dn_2: 0.4582 (0.6078)  loss_bbox_dn_3: 0.4548 (0.6028)  loss_bbox_dn_4: 0.4532 (0.6018)  loss_bbox_dn_5: 0.4533 (0.6021)  loss_giou: 0.7439 (0.9779)  loss_giou_aux_0: 0.7785 (1.0465)  loss_giou_aux_1: 0.7543 (1.0100)  loss_giou_aux_2: 0.7560 (0.9938)  loss_giou_aux_3: 0.7520 (0.9861)  loss_giou_aux_4: 0.7439 (0.9815)  loss_giou_aux_5: 0.9221 (1.1521)  loss_giou_dn_0: 0.9896 (1.1374)  loss_giou_dn_1: 0.8779 (1.0653)  loss_giou_dn_2: 0.8351 (1.0372)  loss_giou_dn_3: 0.8147 (1.0265)  loss_giou_dn_4: 0.8108 (1.0244)  loss_giou_dn_5: 0.8112 (1.0256)  loss_vfl: 0.9718 (1.0049)  loss_vfl_aux_0: 0.9778 (0.9421)  loss_vfl_aux_1: 1.0006 (0.9663)  loss_vfl_aux_2: 0.9713 (0.9769)  loss_vfl_aux_3: 0.9632 (0.9826)  loss_vfl_aux_4: 0.9608 (0.9929)  loss_vfl_aux_5: 0.9521 (0.9171)  loss_vfl_dn_0: 0.4903 (0.5179)  loss_vfl_dn_1: 0.4999 (0.5311)  loss_vfl_dn_2: 0.4996 (0.5467)  loss_vfl_dn_3: 0.5096 (0.5560)  loss_vfl_dn_4: 0.5177 (0.5638)  loss_vfl_dn_5: 0.5180 (0.5674)  time: 0.3472  data: 0.0064  max mem: 12454
