[W619 02:00:57.378123300 Utils.hpp:137] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
/home/hslee/anaconda3/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
[W619 02:00:57.397870963 Utils.hpp:137] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
/home/hslee/anaconda3/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
[rank0]:[W619 02:00:57.554360123 ProcessGroupNCCL.cpp:4718] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can pecify device_id in init_process_group() to force use of a particular device.
[rank1]:[W619 02:00:57.555581646 ProcessGroupNCCL.cpp:4718] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can pecify device_id in init_process_group() to force use of a particular device.
black:413413:413413 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eno1
black:413413:413413 [0] NCCL INFO Bootstrap: Using eno1:117.16.243.135<0>
black:413413:413413 [0] NCCL INFO cudaDriverVersion 12080
black:413413:413413 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
black:413413:413413 [0] NCCL INFO Comm config Blocking set to 1
black:413414:413414 [1] NCCL INFO cudaDriverVersion 12080
black:413414:413414 [1] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eno1
black:413414:413414 [1] NCCL INFO Bootstrap: Using eno1:117.16.243.135<0>
black:413414:413414 [1] NCCL INFO NCCL version 2.26.2+cuda12.2
black:413414:413414 [1] NCCL INFO Comm config Blocking set to 1
black:413414:413454 [1] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
black:413414:413454 [1] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eno1
black:413414:413454 [1] NCCL INFO NET/IB : No device found.
black:413414:413454 [1] NCCL INFO NET/IB : Using [RO]; OOB eno1:117.16.243.135<0>
black:413414:413454 [1] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eno1
black:413414:413454 [1] NCCL INFO NET/Socket : Using [0]eno1:117.16.243.135<0>
black:413414:413454 [1] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
black:413414:413454 [1] NCCL INFO Using network Socket
black:413413:413453 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
black:413413:413453 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eno1
black:413413:413453 [0] NCCL INFO NET/IB : No device found.
black:413413:413453 [0] NCCL INFO NET/IB : Using [RO]; OOB eno1:117.16.243.135<0>
black:413413:413453 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eno1
black:413413:413453 [0] NCCL INFO NET/Socket : Using [0]eno1:117.16.243.135<0>
black:413413:413453 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
black:413413:413453 [0] NCCL INFO Using network Socket
black:413414:413454 [1] NCCL INFO ncclCommInitRankConfig comm 0x41720170 rank 1 nranks 2 cudaDev 1 nvmlDev 1 busId 41000 commId 0x799ba3a146e1fa25 - Init START
black:413413:413453 [0] NCCL INFO ncclCommInitRankConfig comm 0x49ad9c40 rank 0 nranks 2 cudaDev 0 nvmlDev 0 busId 1000 commId 0x799ba3a146e1fa25 - Init START
black:413413:413453 [0] NCCL INFO RAS client listening socket at 127.0.0.1<28028>
black:413414:413454 [1] NCCL INFO RAS client listening socket at 127.0.0.1<28028>
black:413413:413453 [0] NCCL INFO Bootstrap timings total 0.019168 (create 0.000019, send 0.000052, recv 0.000117, ring 0.000737, delay 0.000000)
black:413414:413454 [1] NCCL INFO Bootstrap timings total 0.055972 (create 0.000026, send 0.000065, recv 0.036863, ring 0.018776, delay 0.000000)
black:413414:413454 [1] NCCL INFO NCCL_P2P_DISABLE set by environment to 1
black:413413:413453 [0] NCCL INFO NCCL_P2P_DISABLE set by environment to 1
black:413414:413454 [1] NCCL INFO comm 0x41720170 rank 1 nRanks 2 nNodes 1 localRanks 2 localRank 1 MNNVL 0
black:413413:413453 [0] NCCL INFO comm 0x49ad9c40 rank 0 nRanks 2 nNodes 1 localRanks 2 localRank 0 MNNVL 0
black:413413:413453 [0] NCCL INFO Channel 00/04 : 0 1
black:413414:413454 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1 [2] -1/-1/-1->1->0 [3] 0/-1/-1->1->-1
black:413413:413453 [0] NCCL INFO Channel 01/04 : 0 1
black:413413:413453 [0] NCCL INFO Channel 02/04 : 0 1
black:413414:413454 [1] NCCL INFO P2P Chunksize set to 131072
black:413413:413453 [0] NCCL INFO Channel 03/04 : 0 1
black:413413:413453 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1 [2] 1/-1/-1->0->-1 [3] -1/-1/-1->0->1
black:413413:413453 [0] NCCL INFO P2P Chunksize set to 131072
black:413413:413453 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
black:413414:413621 [1] NCCL INFO [Proxy Service] Device 1 CPU core 55
black:413414:413623 [1] NCCL INFO [Proxy Service UDS] Device 1 CPU core 55
black:413413:413622 [0] NCCL INFO [Proxy Service] Device 0 CPU core 18
black:413413:413624 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 52
black:413414:413454 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 512 | 512
black:413414:413454 [1] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
black:413413:413453 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 512 | 512
black:413413:413453 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
black:413413:413453 [0] NCCL INFO CC Off, workFifoBytes 1048576
black:413413:413453 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
black:413414:413454 [1] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
black:413414:413454 [1] NCCL INFO ncclCommInitRankConfig comm 0x41720170 rank 1 nranks 2 cudaDev 1 nvmlDev 1 busId 41000 commId 0x799ba3a146e1fa25 - Init COMPLETE
black:413413:413453 [0] NCCL INFO ncclCommInitRankConfig comm 0x49ad9c40 rank 0 nranks 2 cudaDev 0 nvmlDev 0 busId 1000 commId 0x799ba3a146e1fa25 - Init COMPLETE
black:413413:413453 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 2 total 20.63 (kernels 20.56, alloc 0.05, bootstrap 0.02, allgathers 0.00, topo 0.00, graphs 0.00, connections 0.00, rest 0.00)
black:413414:413454 [1] NCCL INFO Init timings - ncclCommInitRankConfig: rank 1 nranks 2 total 20.56 (kernels 20.42, alloc 0.08, bootstrap 0.06, allgathers 0.00, topo 0.00, graphs 0.00, connections 0.00, rest 0.00)
black:413413:413626 [0] NCCL INFO Channel 00 : 0[0] -> 1[1] via SHM/direct/direct
black:413414:413625 [1] NCCL INFO Channel 00 : 1[1] -> 0[0] via SHM/direct/direct
black:413413:413626 [0] NCCL INFO Channel 01 : 0[0] -> 1[1] via SHM/direct/direct
black:413414:413625 [1] NCCL INFO Channel 01 : 1[1] -> 0[0] via SHM/direct/direct
black:413413:413626 [0] NCCL INFO Channel 02 : 0[0] -> 1[1] via SHM/direct/direct
black:413414:413625 [1] NCCL INFO Channel 02 : 1[1] -> 0[0] via SHM/direct/direct
black:413413:413626 [0] NCCL INFO Channel 03 : 0[0] -> 1[1] via SHM/direct/direct
black:413414:413625 [1] NCCL INFO Channel 03 : 1[1] -> 0[0] via SHM/direct/direct
black:413414:413625 [1] NCCL INFO Connected all rings, use ring PXN 0 GDR 1
black:413413:413626 [0] NCCL INFO Connected all rings, use ring PXN 0 GDR 1
Initialized distributed mode...
Initialized distributed mode...
cfg:  {'task': 'detection', '_model': None, '_postprocessor': None, '_criterion': None, '_optimizer': None, '_lr_scheduler': None, '_lr_warmup_scheduler': None, '_train_dataloader': None, '_val_dataloader': None, '_ema': None, '_scaler': None, '_train_dataset': None, '_val_dataset': None, '_collate_fn': None, '_evaluator': None, '_writer': None, 'num_workers': 0, 'batch_size': None, '_train_batch_size': None, '_val_batch_size': None, '_train_shuffle': None, '_val_shuffle': None, 'resume': None, 'tuning': None, 'epoches': 72, 'last_epoch': -1, 'use_amp': False, 'use_ema': True, 'ema_decay': 0.9999, 'ema_warmups': 2000, 'sync_bn': True, 'clip_max_norm': 0.1, 'find_unused_parameters': False, 'seed': None, 'print_freq': 100, 'checkpoint_freq': 20, 'output_dir': './output/*v1_r18vd_72e_coco_SemanticAlign-ReLUCosFormer', 'summary_dir': None, 'device': '', 'yaml_cfg': {'task': 'detection', 'evaluator': {'type': 'CocoEvaluator', 'iou_types': ['bbox']}, 'num_classes': 80, 'remap_mscoco_category': True, 'train_dataloader': {'type': 'DataLoader', 'dataset': {'type': 'CocoDetection', 'img_folder': '/media/data/coco/images/train2017', 'ann_file': '/media/data/coco/annotations/instances_train2017.json', 'return_masks': False, 'transforms': {'type': 'Compose', 'ops': [{'type': 'RandomPhotometricDistort', 'p': 0.5}, {'type': 'RandomZoomOut', 'fill': 0}, {'type': 'RandomIoUCrop', 'p': 0.8}, {'type': 'SanitizeBoundingBoxes', 'min_size': 1}, {'type': 'RandomHorizontalFlip'}, {'type': 'Resize', 'size': [640, 640]}, {'type': 'SanitizeBoundingBoxes', 'min_size': 1}, {'type': 'ConvertPILImage', 'dtype': 'float32', 'scale': True}, {'type': 'ConvertBoxes', 'fmt': 'cxcywh', 'normalize': True}]}}, 'shuffle': True, 'num_workers': 4, 'drop_last': True, 'collate_fn': {'type': 'BatchImageCollateFuncion', 'scales': [480, 512, 544, 576, 608, 640, 640, 640, 672, 704, 736, 768, 800]}, 'total_batch_size': 16}, 'val_dataloader': {'type': 'DataLoader', 'dataset': {'type': 'CocoDetection', 'img_folder': '/media/data/coco/images/val2017/', 'ann_file': '/media/data/coco/annotations/instances_val2017.json', 'return_masks': False, 'transforms': {'type': 'Compose', 'ops': [{'type': 'Resize', 'size': [640, 640]}, {'type': 'ConvertPILImage', 'dtype': 'float32', 'scale': True}]}}, 'shuffle': False, 'num_workers': 8, 'drop_last': False, 'collate_fn': {'type': 'BatchImageCollateFuncion'}, 'total_batch_size': 16}, 'print_freq': 100, 'output_dir': './output/*v1_r18vd_72e_coco_SemanticAlign-ReLUCosFormer', 'checkpoint_freq': 20, 'sync_bn': True, 'find_unused_parameters': False, 'use_amp': False, 'scaler': {'type': 'GradScaler', 'enabled': True}, 'use_ema': True, 'ema': {'type': 'ModelEMA', 'decay': 0.9999, 'warmups': 2000}, 'epoches': 72, 'clip_max_norm': 0.1, 'optimizer': {'type': 'AdamW', 'params': [{'params': '^(?=.*backbone)(?=.*norm|bn).*$', 'weight_decay': 0.0, 'lr': 1e-05}, {'params': '^(?=.*backbone)(?!.*norm|bn).*$', 'lr': 1e-05}, {'params': '^(?=.*(?:encoder|decoder))(?=.*(?:norm|bn|bias)).*$', 'weight_decay': 0.0}], 'lr': 0.0001, 'betas': [0.9, 0.999], 'weight_decay': 0.0001}, 'lr_scheduler': {'type': 'MultiStepLR', 'milestones': [1000], 'gamma': 0.1}, 'lr_warmup_scheduler': {'type': 'LinearWarmup', 'warmup_duration': 2000}, 'model': 'RTDETR', 'criterion': 'RTDETRCriterion', 'postprocessor': 'RTDETRPostProcessor', 'use_focal_loss': True, 'eval_spatial_size': [640, 640], 'RTDETR': {'backbone': 'PResNet', 'encoder': 'HybridEncoder', 'decoder': 'RTDETRTransformer'}, 'PResNet': {'depth': 18, 'variant': 'd', 'freeze_at': -1, 'return_idx': [1, 2, 3], 'num_stages': 4, 'freeze_norm': False, 'pretrained': True}, 'HybridEncoder': {'in_channels': [128, 256, 512], 'feat_strides': [8, 16, 32], 'hidden_dim': 256, 'use_encoder_idx': [2], 'num_encoder_layers': 1, 'nhead': 8, 'dim_feedforward': 1024, 'dropout': 0.0, 'enc_act': 'gelu', 'expansion': 0.5, 'depth_mult': 1, 'act': 'silu', 'version': 'v1'}, 'RTDETRTransformer': {'feat_channels': [256, 256, 256], 'feat_strides': [8, 16, 32], 'hidden_dim': 256, 'num_levels': 3, 'num_layers': 3, 'num_queries': 300, 'num_denoising': 100, 'label_noise_ratio': 0.5, 'box_noise_scale': 1.0, 'eval_idx': -1}, 'RTDETRPostProcessor': {'num_top_queries': 300}, 'RTDETRCriterion': {'weight_dict': {'loss_vfl': 1, 'loss_bbox': 5, 'loss_giou': 2}, 'losses': ['vfl', 'boxes'], 'alpha': 0.75, 'gamma': 2.0, 'matcher': {'type': 'HungarianMatcher', 'weight_dict': {'cost_class': 2, 'cost_bbox': 5, 'cost_giou': 2}, 'alpha': 0.25, 'gamma': 2.0}}, '__include__': ['../dataset/coco_detection.yml', '../runtime.yml', './include/dataloader.yml', './include/optimizer.yml', './include/rtdetr_r50vd.yml'], 'config': '/home/hslee/FINE-FPN/rtdetrv2_pytorch2/configs/rtdetr/rtdetr_r18vd_6x_coco.yml', 'test_only': False, 'print_method': 'builtin', 'print_rank': 0}}
Start training
Load PResNet18 state_dict
model: RTDETR(
  (backbone): PResNet(
    (conv1): Sequential(
      (conv1_1): ConvNormLayer(
        (conv): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (norm): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): ReLU(inplace=True)
      )
      (conv1_2): ConvNormLayer(
        (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (norm): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): ReLU(inplace=True)
      )
      (conv1_3): ConvNormLayer(
        (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (norm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): ReLU(inplace=True)
      )
    )
    (res_layers): ModuleList(
      (0): Blocks(
        (blocks): ModuleList(
          (0): BasicBlock(
            (short): ConvNormLayer(
              (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (norm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act): Identity()
            )
            (branch2a): ConvNormLayer(
              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act): ReLU(inplace=True)
            )
            (branch2b): ConvNormLayer(
              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act): Identity()
            )
            (act): ReLU(inplace=True)
          )
          (1): BasicBlock(
            (branch2a): ConvNormLayer(
              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act): ReLU(inplace=True)
            )
            (branch2b): ConvNormLayer(
              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act): Identity()
            )
            (act): ReLU(inplace=True)
          )
        )
      )
      (1): Blocks(
        (blocks): ModuleList(
          (0): BasicBlock(
            (short): Sequential(
              (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)
              (conv): ConvNormLayer(
                (conv): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (norm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (act): Identity()
              )
            )
            (branch2a): ConvNormLayer(
              (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
              (norm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act): ReLU(inplace=True)
            )
            (branch2b): ConvNormLayer(
              (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act): Identity()
            )
            (act): ReLU(inplace=True)
          )
          (1): BasicBlock(
            (branch2a): ConvNormLayer(
              (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act): ReLU(inplace=True)
            )
            (branch2b): ConvNormLayer(
              (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act): Identity()
            )
            (act): ReLU(inplace=True)
          )
        )
      )
      (2): Blocks(
        (blocks): ModuleList(
          (0): BasicBlock(
            (short): Sequential(
              (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)
              (conv): ConvNormLayer(
                (conv): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (act): Identity()
              )
            )
            (branch2a): ConvNormLayer(
              (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
              (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act): ReLU(inplace=True)
            )
            (branch2b): ConvNormLayer(
              (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act): Identity()
            )
            (act): ReLU(inplace=True)
          )
          (1): BasicBlock(
            (branch2a): ConvNormLayer(
              (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act): ReLU(inplace=True)
            )
            (branch2b): ConvNormLayer(
              (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act): Identity()
            )
            (act): ReLU(inplace=True)
          )
        )
      )
      (3): Blocks(
        (blocks): ModuleList(
          (0): BasicBlock(
            (short): Sequential(
              (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)
              (conv): ConvNormLayer(
                (conv): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (norm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (act): Identity()
              )
            )
            (branch2a): ConvNormLayer(
              (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
              (norm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act): ReLU(inplace=True)
            )
            (branch2b): ConvNormLayer(
              (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act): Identity()
            )
            (act): ReLU(inplace=True)
          )
          (1): BasicBlock(
            (branch2a): ConvNormLayer(
              (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act): ReLU(inplace=True)
            )
            (branch2b): ConvNormLayer(
              (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act): Identity()
            )
            (act): ReLU(inplace=True)
          )
        )
      )
    )
  )
  (decoder): RTDETRTransformer(
    (input_proj): ModuleList(
      (0-2): 3 x Sequential(
        (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (decoder): TransformerDecoder(
      (layers): ModuleList(
        (0-2): 3 x TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (dropout1): Dropout(p=0.0, inplace=False)
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (cross_attn): MSDeformableAttention(
            (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
            (attention_weights): Linear(in_features=256, out_features=96, bias=True)
            (value_proj): Linear(in_features=256, out_features=256, bias=True)
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
          (dropout2): Dropout(p=0.0, inplace=False)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (linear1): Linear(in_features=256, out_features=1024, bias=True)
          (dropout3): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout4): Dropout(p=0.0, inplace=False)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (denoising_class_embed): Embedding(81, 256, padding_idx=80)
    (query_pos_head): MLP(
      (layers): ModuleList(
        (0): Linear(in_features=4, out_features=512, bias=True)
        (1): Linear(in_features=512, out_features=256, bias=True)
      )
      (act): ReLU(inplace=True)
    )
    (enc_output): Sequential(
      (0): Linear(in_features=256, out_features=256, bias=True)
      (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
    (enc_score_head): Linear(in_features=256, out_features=80, bias=True)
    (enc_bbox_head): MLP(
      (layers): ModuleList(
        (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)
        (2): Linear(in_features=256, out_features=4, bias=True)
      )
      (act): ReLU(inplace=True)
    )
    (dec_score_head): ModuleList(
      (0-2): 3 x Linear(in_features=256, out_features=80, bias=True)
    )
    (dec_bbox_head): ModuleList(
      (0-2): 3 x MLP(
        (layers): ModuleList(
          (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
        (act): ReLU(inplace=True)
      )
    )
  )
  (encoder): HybridEncoder(
    (input_proj): ModuleList(
      (0): Sequential(
        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): Sequential(
        (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): Sequential(
        (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (encoder): ModuleList(
      (0): TransformerEncoder(
        (layers): ModuleList(
          (0): TransformerEncoderLayer(
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout1): Dropout(p=0.0, inplace=False)
            (dropout2): Dropout(p=0.0, inplace=False)
            (activation): GELU(approximate='none')
          )
        )
      )
    )
    (lateral_convs): ModuleList(
      (0-1): 2 x ConvNormLayer(
        (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): SiLU(inplace=True)
      )
    )
    (fpn_blocks): ModuleList(
      (0-1): 2 x CSPRepLayer(
        (conv1): ConvNormLayer(
          (conv): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (norm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act): SiLU(inplace=True)
        )
        (conv2): ConvNormLayer(
          (conv): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (norm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act): SiLU(inplace=True)
        )
        (bottlenecks): Sequential(
          (0): RepVggBlock(
            (conv1): ConvNormLayer(
              (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act): Identity()
            )
            (conv2): ConvNormLayer(
              (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (norm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act): Identity()
            )
            (act): SiLU(inplace=True)
          )
          (1): RepVggBlock(
            (conv1): ConvNormLayer(
              (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act): Identity()
            )
            (conv2): ConvNormLayer(
              (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (norm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act): Identity()
            )
            (act): SiLU(inplace=True)
          )
          (2): RepVggBlock(
            (conv1): ConvNormLayer(
              (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act): Identity()
            )
            (conv2): ConvNormLayer(
              (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (norm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act): Identity()
            )
            (act): SiLU(inplace=True)
          )
        )
        (conv3): ConvNormLayer(
          (conv): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act): SiLU(inplace=True)
        )
      )
    )
    (downsample_convs): ModuleList(
      (0-1): 2 x ConvNormLayer(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): SiLU(inplace=True)
      )
    )
    (pan_blocks): ModuleList(
      (0-1): 2 x CSPRepLayer(
        (conv1): ConvNormLayer(
          (conv): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (norm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act): SiLU(inplace=True)
        )
        (conv2): ConvNormLayer(
          (conv): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (norm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act): SiLU(inplace=True)
        )
        (bottlenecks): Sequential(
          (0): RepVggBlock(
            (conv1): ConvNormLayer(
              (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act): Identity()
            )
            (conv2): ConvNormLayer(
              (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (norm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act): Identity()
            )
            (act): SiLU(inplace=True)
          )
          (1): RepVggBlock(
            (conv1): ConvNormLayer(
              (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act): Identity()
            )
            (conv2): ConvNormLayer(
              (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (norm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act): Identity()
            )
            (act): SiLU(inplace=True)
          )
          (2): RepVggBlock(
            (conv1): ConvNormLayer(
              (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act): Identity()
            )
            (conv2): ConvNormLayer(
              (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (norm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act): Identity()
            )
            (act): SiLU(inplace=True)
          )
        )
        (conv3): ConvNormLayer(
          (conv): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act): SiLU(inplace=True)
        )
      )
    )
    (sa_cross_attn): ModuleList(
      (0-1): 2 x SpatialAlignCrossAttentionCosFormer(
        (q_proj): Sequential(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (k_proj): Sequential(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (v_proj): Linear(in_features=256, out_features=256, bias=True)
        (out_proj): Linear(in_features=256, out_features=256, bias=True)
        (linear1): Linear(in_features=256, out_features=1024, bias=True)
        (dropout): Dropout(p=0.0, inplace=False)
        (linear2): Linear(in_features=1024, out_features=256, bias=True)
        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (activation): GELU(approximate='none')
        (avg_pool_2x): AvgPool2d(kernel_size=2, stride=2, padding=0)
        (avg_pool_4x): AvgPool2d(kernel_size=4, stride=4, padding=0)
      )
    )
  )
)

[2025-06-19 02:01:18] black:413413:413413 [0] enqueue.cc:1556 NCCL WARN Cuda failure 700 'an illegal memory access was encountered'
black:413413:413413 [0] NCCL INFO group.cc:241 -> 1
black:413413:413413 [0] NCCL INFO group.cc:478 -> 1
black:413413:413413 [0] NCCL INFO group.cc:581 -> 1
black:413413:413413 [0] NCCL INFO enqueue.cc:2299 -> 1

[2025-06-19 02:01:18] black:413414:413414 [1] enqueue.cc:1556 NCCL WARN Cuda failure 700 'an illegal memory access was encountered'
black:413414:413414 [1] NCCL INFO group.cc:241 -> 1
black:413414:413414 [1] NCCL INFO group.cc:478 -> 1
black:413414:413414 [1] NCCL INFO group.cc:581 -> 1
black:413414:413414 [1] NCCL INFO enqueue.cc:2299 -> 1
W0619 02:01:19.973000 413410 site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 413414 closing signal SIGTERM
E0619 02:01:21.043000 413410 site-packages/torch/distributed/elastic/multiprocessing/api.py:874] failed (exitcode: -6) local_rank: 0 (pid: 413413) of binary: /home/hslee/anaconda3/bin/python
Traceback (most recent call last):
  File "/home/hslee/anaconda3/bin/torchrun", line 8, in <module>
    sys.exit(main())
             ^^^^^^
  File "/home/hslee/anaconda3/lib/python3.12/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 355, in wrapper
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File "/home/hslee/anaconda3/lib/python3.12/site-packages/torch/distributed/run.py", line 892, in main
    run(args)
  File "/home/hslee/anaconda3/lib/python3.12/site-packages/torch/distributed/run.py", line 883, in run
    elastic_launch(
  File "/home/hslee/anaconda3/lib/python3.12/site-packages/torch/distributed/launcher/api.py", line 139, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hslee/anaconda3/lib/python3.12/site-packages/torch/distributed/launcher/api.py", line 270, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
=======================================================
tools/train.py FAILED
-------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
-------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-06-19_02:01:19
  host      : black
  rank      : 0 (local_rank: 0)
  exitcode  : -6 (pid: 413413)
  error_file: <N/A>
  traceback : Signal 6 (SIGABRT) received by PID 413413
=======================================================
